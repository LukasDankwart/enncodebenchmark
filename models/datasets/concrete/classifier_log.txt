/usr/bin/python3.10 /home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py
/home/mustafa/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
Epoch 1/500, Training Loss: 0.6989257454706792
Epoch 1/500, Validation Loss: 0.6945097879929976
Epoch 2/500, Training Loss: 0.6926858450767898
Epoch 2/500, Validation Loss: 0.6889291846906984
Epoch 3/500, Training Loss: 0.6873626699064046
Epoch 3/500, Validation Loss: 0.6834762313149192
Epoch 4/500, Training Loss: 0.6814437282068885
Epoch 4/500, Validation Loss: 0.6751401548261766
Epoch 5/500, Training Loss: 0.6696128843561455
Epoch 5/500, Validation Loss: 0.6534962282552348
Epoch 6/500, Training Loss: 0.6441720176007645
Epoch 6/500, Validation Loss: 0.6213574184999837
Epoch 7/500, Training Loss: 0.6094943928983109
Epoch 7/500, Validation Loss: 0.5778427402694504
Epoch 8/500, Training Loss: 0.5646670531796681
Epoch 8/500, Validation Loss: 0.527660841291601
Epoch 9/500, Training Loss: 0.5247284690649268
Epoch 9/500, Validation Loss: 0.4878550694360361
Epoch 10/500, Training Loss: 0.49082232083758437
Epoch 10/500, Validation Loss: 0.4528532032068674
Epoch 11/500, Training Loss: 0.46904971926949723
Epoch 11/500, Validation Loss: 0.42985147857046746
Epoch 12/500, Training Loss: 0.4550535842548958
Epoch 12/500, Validation Loss: 0.4127702500138964
Epoch 13/500, Training Loss: 0.43690300973625684
Epoch 13/500, Validation Loss: 0.39721022062487416
Epoch 14/500, Training Loss: 0.425677475792029
Epoch 14/500, Validation Loss: 0.38475634138305465
Epoch 15/500, Training Loss: 0.41449675075554815
Epoch 15/500, Validation Loss: 0.3769473839889873
Epoch 16/500, Training Loss: 0.40970886444954535
Epoch 16/500, Validation Loss: 0.3703566522567303
Epoch 17/500, Training Loss: 0.40098522771950734
Epoch 17/500, Validation Loss: 0.3697557480304272
Epoch 18/500, Training Loss: 0.3946753961037994
Epoch 18/500, Validation Loss: 0.35799217301529723
Epoch 19/500, Training Loss: 0.38884469722081155
Epoch 19/500, Validation Loss: 0.35179947297294417
Epoch 20/500, Training Loss: 0.3850494275972681
Epoch 20/500, Validation Loss: 0.3498874713074077
Epoch 21/500, Training Loss: 0.37911033861845445
Epoch 21/500, Validation Loss: 0.34762150049209595
Epoch 22/500, Training Loss: 0.37897169829737626
Epoch 22/500, Validation Loss: 0.34057420917919706
Epoch 23/500, Training Loss: 0.37293311617700137
Epoch 23/500, Validation Loss: 0.33973172539240354
Epoch 24/500, Training Loss: 0.3697633807664936
Epoch 24/500, Validation Loss: 0.33970953040308766
Epoch 25/500, Training Loss: 0.3688248912603614
Epoch 25/500, Validation Loss: 0.3377987904982133
Epoch 26/500, Training Loss: 0.3682115145761328
Epoch 26/500, Validation Loss: 0.3348407756972623
Epoch 27/500, Training Loss: 0.3661005093552038
Epoch 27/500, Validation Loss: 0.3369593419037856
Epoch 28/500, Training Loss: 0.366928011509648
Epoch 28/500, Validation Loss: 0.3347649175625343
Epoch 29/500, Training Loss: 0.3614146904335273
Epoch 29/500, Validation Loss: 0.33752246684842296
Epoch 30/500, Training Loss: 0.36064814030090747
Epoch 30/500, Validation Loss: 0.3319746081705217
Epoch 31/500, Training Loss: 0.3599864111208304
Epoch 31/500, Validation Loss: 0.3314101703755267
Epoch 32/500, Training Loss: 0.3581139651978396
Epoch 32/500, Validation Loss: 0.330469504192278
Epoch 33/500, Training Loss: 0.3595325855044816
Epoch 33/500, Validation Loss: 0.3309187776856608
Epoch 34/500, Training Loss: 0.3552952880548745
Epoch 34/500, Validation Loss: 0.3293229662752771
Epoch 35/500, Training Loss: 0.3541230432658189
Epoch 35/500, Validation Loss: 0.3279391202059659
Epoch 36/500, Training Loss: 0.3542394599413574
Epoch 36/500, Validation Loss: 0.32745064272508995
Epoch 37/500, Training Loss: 0.3515470791375587
Epoch 37/500, Validation Loss: 0.32737727056850086
Epoch 38/500, Training Loss: 0.35484964072952324
Epoch 38/500, Validation Loss: 0.3283696886780974
Epoch 39/500, Training Loss: 0.3501014231643465
Epoch 39/500, Validation Loss: 0.32515749335289
Epoch 40/500, Training Loss: 0.3579953763422126
Epoch 40/500, Validation Loss: 0.3246237465313503
Epoch 41/500, Training Loss: 0.3543805182600154
Epoch 41/500, Validation Loss: 0.32967949455434625
Epoch 42/500, Training Loss: 0.3525577369514186
Epoch 42/500, Validation Loss: 0.3278818269828697
Epoch 43/500, Training Loss: 0.3486837835649181
Epoch 43/500, Validation Loss: 0.3237381794235923
Epoch 44/500, Training Loss: 0.34661950361650135
Epoch 44/500, Validation Loss: 0.32265126859986937
Epoch 45/500, Training Loss: 0.34563319106042467
Epoch 45/500, Validation Loss: 0.3226632189441037
Epoch 46/500, Training Loss: 0.35496190378388154
Epoch 46/500, Validation Loss: 0.3288274767336907
Epoch 47/500, Training Loss: 0.34671013671052275
Epoch 47/500, Validation Loss: 0.3226178683243789
Epoch 48/500, Training Loss: 0.345609111564128
Epoch 48/500, Validation Loss: 0.3211849161556789
Epoch 49/500, Training Loss: 0.346802445849334
Epoch 49/500, Validation Loss: 0.3210412163239021
Epoch 50/500, Training Loss: 0.3480396122773709
Epoch 50/500, Validation Loss: 0.3272637895175389
Epoch 51/500, Training Loss: 0.35132668435253234
Epoch 51/500, Validation Loss: 0.32166885749086155
Epoch 52/500, Training Loss: 0.34335545763559383
Epoch 52/500, Validation Loss: 0.32194007139701347
Epoch 53/500, Training Loss: 0.34561970462084807
Epoch 53/500, Validation Loss: 0.3219400388853891
Epoch 54/500, Training Loss: 0.3424216319526573
Epoch 54/500, Validation Loss: 0.3209403683612873
Epoch 55/500, Training Loss: 0.3414470929543949
Epoch 55/500, Validation Loss: 0.3216975532568894
Epoch 56/500, Training Loss: 0.35063690649818285
Epoch 56/500, Validation Loss: 0.32128300411360605
Epoch 57/500, Training Loss: 0.34220639859092383
Epoch 57/500, Validation Loss: 0.3228458220308477
Epoch 58/500, Training Loss: 0.3455137997328326
Epoch 58/500, Validation Loss: 0.32104611280676604
Epoch 59/500, Training Loss: 0.3413158472053882
Epoch 59/500, Validation Loss: 0.3221928368915211
Epoch 60/500, Training Loss: 0.3466021213758008
Epoch 60/500, Validation Loss: 0.32798361971780854
Epoch 61/500, Training Loss: 0.34362722308621657
Epoch 61/500, Validation Loss: 0.32225433301616024
Epoch 62/500, Training Loss: 0.3395602440619105
Epoch 62/500, Validation Loss: 0.32026607417441033
Epoch 63/500, Training Loss: 0.3401543615429808
Epoch 63/500, Validation Loss: 0.3187849111371226
Epoch 64/500, Training Loss: 0.33907579083961853
Epoch 64/500, Validation Loss: 0.3188458417917227
Epoch 65/500, Training Loss: 0.3400215839298026
Epoch 65/500, Validation Loss: 0.3178223971422617
Epoch 66/500, Training Loss: 0.3387339363151053
Epoch 66/500, Validation Loss: 0.3175317641202505
Epoch 67/500, Training Loss: 0.3409451010188762
Epoch 67/500, Validation Loss: 0.3175980224237814
Epoch 68/500, Training Loss: 0.3376242649042656
Epoch 68/500, Validation Loss: 0.31672119010578503
Epoch 69/500, Training Loss: 0.340098333269895
Epoch 69/500, Validation Loss: 0.31629530098531156
Epoch 70/500, Training Loss: 0.3378343913450982
Epoch 70/500, Validation Loss: 0.3161635704628833
Epoch 71/500, Training Loss: 0.33575931803198034
Epoch 71/500, Validation Loss: 0.31501344310772883
Epoch 72/500, Training Loss: 0.33811024509256654
Epoch 72/500, Validation Loss: 0.31427811260347244
Epoch 73/500, Training Loss: 0.33488226087761325
Epoch 73/500, Validation Loss: 0.31465666402469983
Epoch 74/500, Training Loss: 0.3341179384479245
Epoch 74/500, Validation Loss: 0.314165001952803
Epoch 75/500, Training Loss: 0.3344951378034322
Epoch 75/500, Validation Loss: 0.31438540057702496
Epoch 76/500, Training Loss: 0.3351898244614409
Epoch 76/500, Validation Loss: 0.3125601774686343
Epoch 77/500, Training Loss: 0.3329468339296696
Epoch 77/500, Validation Loss: 0.3146290248864657
Epoch 78/500, Training Loss: 0.33600149828625453
Epoch 78/500, Validation Loss: 0.3120148228360461
Epoch 79/500, Training Loss: 0.337078919585138
Epoch 79/500, Validation Loss: 0.31407031455597323
Epoch 80/500, Training Loss: 0.3326523808630677
Epoch 80/500, Validation Loss: 0.31129307057950406
Epoch 81/500, Training Loss: 0.3311331323380278
Epoch 81/500, Validation Loss: 0.3108887869816322
Epoch 82/500, Training Loss: 0.33314747544168266
Epoch 82/500, Validation Loss: 0.3102543609482901
Epoch 83/500, Training Loss: 0.33163016669751866
Epoch 83/500, Validation Loss: 0.30977851307237303
Epoch 84/500, Training Loss: 0.3308431349911736
Epoch 84/500, Validation Loss: 0.30969429828903894
Epoch 85/500, Training Loss: 0.32994041741721014
Epoch 85/500, Validation Loss: 0.30947207204707256
Epoch 86/500, Training Loss: 0.3305066987644783
Epoch 86/500, Validation Loss: 0.30959861696540536
Epoch 87/500, Training Loss: 0.3302013613851986
Epoch 87/500, Validation Loss: 0.3078207195579232
Epoch 88/500, Training Loss: 0.3287916456332402
Epoch 88/500, Validation Loss: 0.3078804240598307
Epoch 89/500, Training Loss: 0.3277821878511226
Epoch 89/500, Validation Loss: 0.30722547738583056
Epoch 90/500, Training Loss: 0.327822113277884
Epoch 90/500, Validation Loss: 0.30692048157964436
Epoch 91/500, Training Loss: 0.32799637419960864
Epoch 91/500, Validation Loss: 0.306945340199904
Epoch 92/500, Training Loss: 0.3289756547664802
Epoch 92/500, Validation Loss: 0.3057248058257165
Epoch 93/500, Training Loss: 0.32595621828563337
Epoch 93/500, Validation Loss: 0.3048341510357795
Epoch 94/500, Training Loss: 0.3355023286534001
Epoch 94/500, Validation Loss: 0.30079735602651325
Epoch 95/500, Training Loss: 0.3268368297079565
Epoch 95/500, Validation Loss: 0.30108393438450703
Epoch 96/500, Training Loss: 0.3286783618506318
Epoch 96/500, Validation Loss: 0.30190785364671185
Epoch 97/500, Training Loss: 0.32367954422795986
Epoch 97/500, Validation Loss: 0.300557436107041
Epoch 98/500, Training Loss: 0.32257890767429476
Epoch 98/500, Validation Loss: 0.3022396963138085
Epoch 99/500, Training Loss: 0.32243087619088395
Epoch 99/500, Validation Loss: 0.30142631043087353
Epoch 100/500, Training Loss: 0.3233831086081043
Epoch 100/500, Validation Loss: 0.2984009381238516
Epoch 101/500, Training Loss: 0.323157178334751
Epoch 101/500, Validation Loss: 0.2981396085256106
Epoch 102/500, Training Loss: 0.32100320457585474
Epoch 102/500, Validation Loss: 0.29827529501605343
Epoch 103/500, Training Loss: 0.3334996797010075
Epoch 103/500, Validation Loss: 0.30129540392330717
Epoch 104/500, Training Loss: 0.3200716365557603
Epoch 104/500, Validation Loss: 0.299804800129556
Epoch 105/500, Training Loss: 0.32635804260570694
Epoch 105/500, Validation Loss: 0.296827505548279
Epoch 106/500, Training Loss: 0.3201126065234371
Epoch 106/500, Validation Loss: 0.3010618903420188
Epoch 107/500, Training Loss: 0.32411993746535955
Epoch 107/500, Validation Loss: 0.2957752734035641
Epoch 108/500, Training Loss: 0.32073039583662905
Epoch 108/500, Validation Loss: 0.2966205297739475
Epoch 109/500, Training Loss: 0.3172305918268952
Epoch 109/500, Validation Loss: 0.300926217011043
Epoch 110/500, Training Loss: 0.3173400795244807
Epoch 110/500, Validation Loss: 0.29516520631777776
Epoch 111/500, Training Loss: 0.31646045699364267
Epoch 111/500, Validation Loss: 0.29488718219391713
Epoch 112/500, Training Loss: 0.3210595494831154
Epoch 112/500, Validation Loss: 0.2948810523980624
Epoch 113/500, Training Loss: 0.3189547407478301
Epoch 113/500, Validation Loss: 0.2976183376529
Epoch 114/500, Training Loss: 0.3181516219333496
Epoch 114/500, Validation Loss: 0.2944044631409955
Epoch 115/500, Training Loss: 0.3143234919245134
Epoch 115/500, Validation Loss: 0.2929012252912893
Epoch 116/500, Training Loss: 0.3165776866814961
Epoch 116/500, Validation Loss: 0.2930541268803857
Epoch 117/500, Training Loss: 0.3155393633531631
Epoch 117/500, Validation Loss: 0.29752661655475565
Epoch 118/500, Training Loss: 0.3162856571057917
Epoch 118/500, Validation Loss: 0.2939304949401261
Epoch 119/500, Training Loss: 0.31319928140445363
Epoch 119/500, Validation Loss: 0.29145760389117453
Epoch 120/500, Training Loss: 0.312884936705955
Epoch 120/500, Validation Loss: 0.2907577193789668
Epoch 121/500, Training Loss: 0.3099214860526186
Epoch 121/500, Validation Loss: 0.28962312309772936
Epoch 122/500, Training Loss: 0.31220056957742215
Epoch 122/500, Validation Loss: 0.29093953193008126
Epoch 123/500, Training Loss: 0.32306157763473864
Epoch 123/500, Validation Loss: 0.2882886416726298
Epoch 124/500, Training Loss: 0.309351237189159
Epoch 124/500, Validation Loss: 0.2876329750983746
Epoch 125/500, Training Loss: 0.30746649716168273
Epoch 125/500, Validation Loss: 0.2870242500847036
Epoch 126/500, Training Loss: 0.3069399200100839
Epoch 126/500, Validation Loss: 0.28743133103692686
Epoch 127/500, Training Loss: 0.30745447935515835
Epoch 127/500, Validation Loss: 0.287179734799769
Epoch 128/500, Training Loss: 0.30449111050772437
Epoch 128/500, Validation Loss: 0.28473022231807954
Epoch 129/500, Training Loss: 0.3038999689342493
Epoch 129/500, Validation Loss: 0.28453438138806975
Epoch 130/500, Training Loss: 0.30344025408345354
Epoch 130/500, Validation Loss: 0.2865624145253912
Epoch 131/500, Training Loss: 0.3019051817032649
Epoch 131/500, Validation Loss: 0.2820892053378093
Epoch 132/500, Training Loss: 0.30269296867217504
Epoch 132/500, Validation Loss: 0.2813245270933424
Epoch 133/500, Training Loss: 0.3009489524629014
Epoch 133/500, Validation Loss: 0.2795345260725393
Epoch 134/500, Training Loss: 0.3006974016413113
Epoch 134/500, Validation Loss: 0.2774182020069717
Epoch 135/500, Training Loss: 0.2982696252881489
Epoch 135/500, Validation Loss: 0.27767734829481544
Epoch 136/500, Training Loss: 0.2988402700802794
Epoch 136/500, Validation Loss: 0.2800439050445309
Epoch 137/500, Training Loss: 0.298399742132107
Epoch 137/500, Validation Loss: 0.27618869577909444
Epoch 138/500, Training Loss: 0.29585801662254596
Epoch 138/500, Validation Loss: 0.2788274861001349
Epoch 139/500, Training Loss: 0.30798963948195585
Epoch 139/500, Validation Loss: 0.2741673956443737
Epoch 140/500, Training Loss: 0.2975155567277569
Epoch 140/500, Validation Loss: 0.2747799231634512
Epoch 141/500, Training Loss: 0.29257687700501595
Epoch 141/500, Validation Loss: 0.27937499927235887
Epoch 142/500, Training Loss: 0.29881404160461217
Epoch 142/500, Validation Loss: 0.2796746908457248
Epoch 143/500, Training Loss: 0.29618216471309966
Epoch 143/500, Validation Loss: 0.27101625734335416
Epoch 144/500, Training Loss: 0.2910494330996614
Epoch 144/500, Validation Loss: 0.2710388244746567
Epoch 145/500, Training Loss: 0.29089434963654875
Epoch 145/500, Validation Loss: 0.2698559836520777
Epoch 146/500, Training Loss: 0.29712879816883975
Epoch 146/500, Validation Loss: 0.2715833560599909
Epoch 147/500, Training Loss: 0.2896889366839861
Epoch 147/500, Validation Loss: 0.26700771648388405
Epoch 148/500, Training Loss: 0.28962334212892266
Epoch 148/500, Validation Loss: 0.2681291952922747
Epoch 149/500, Training Loss: 0.2915837473046143
Epoch 149/500, Validation Loss: 0.26419577776611625
Epoch 150/500, Training Loss: 0.2850324505896162
Epoch 150/500, Validation Loss: 0.2652791492737733
Epoch 151/500, Training Loss: 0.285700556276172
Epoch 151/500, Validation Loss: 0.26618766978189545
Epoch 152/500, Training Loss: 0.2843477008857825
Epoch 152/500, Validation Loss: 0.26235765721890836
Epoch 153/500, Training Loss: 0.2854780511369983
Epoch 153/500, Validation Loss: 0.263121098279953
Epoch 154/500, Training Loss: 0.2881188989512302
Epoch 154/500, Validation Loss: 0.2689372757812599
Epoch 155/500, Training Loss: 0.2876724191165003
Epoch 155/500, Validation Loss: 0.260870253497904
Epoch 156/500, Training Loss: 0.2808458237765401
Epoch 156/500, Validation Loss: 0.2616811003777888
Epoch 157/500, Training Loss: 0.28119827350201126
Epoch 157/500, Validation Loss: 0.2629911346094949
Epoch 158/500, Training Loss: 0.2864436038805949
Epoch 158/500, Validation Loss: 0.26133101230317896
Epoch 159/500, Training Loss: 0.27916135716454826
Epoch 159/500, Validation Loss: 0.26157572633260257
Epoch 160/500, Training Loss: 0.28014166702757926
Epoch 160/500, Validation Loss: 0.2575118247564737
Epoch 161/500, Training Loss: 0.2773910138708244
Epoch 161/500, Validation Loss: 0.2565655592200044
Epoch 162/500, Training Loss: 0.2773591264875968
Epoch 162/500, Validation Loss: 0.25683446576843016
Epoch 163/500, Training Loss: 0.2751590826971025
Epoch 163/500, Validation Loss: 0.2551264108775498
Epoch 164/500, Training Loss: 0.2748854189243132
Epoch 164/500, Validation Loss: 0.2555380030111833
Epoch 165/500, Training Loss: 0.28160175821652195
Epoch 165/500, Validation Loss: 0.2542197187612583
Epoch 166/500, Training Loss: 0.274219345802573
Epoch 166/500, Validation Loss: 0.2584673367537461
Epoch 167/500, Training Loss: 0.2784011109568044
Epoch 167/500, Validation Loss: 0.25280091263257065
Epoch 168/500, Training Loss: 0.271724869676668
Epoch 168/500, Validation Loss: 0.25256163249542185
Epoch 169/500, Training Loss: 0.26990940174844824
Epoch 169/500, Validation Loss: 0.2501443312926726
Epoch 170/500, Training Loss: 0.27276821211290425
Epoch 170/500, Validation Loss: 0.24914148372489137
Epoch 171/500, Training Loss: 0.274152901958824
Epoch 171/500, Validation Loss: 0.25556029843819605
Epoch 172/500, Training Loss: 0.27186140355533933
Epoch 172/500, Validation Loss: 0.25028421004097184
Epoch 173/500, Training Loss: 0.2677320651929624
Epoch 173/500, Validation Loss: 0.24755106653485978
Epoch 174/500, Training Loss: 0.269108018955778
Epoch 174/500, Validation Loss: 0.24751118367368524
Epoch 175/500, Training Loss: 0.26575296471378507
Epoch 175/500, Validation Loss: 0.2453761451043092
Epoch 176/500, Training Loss: 0.26593732724325336
Epoch 176/500, Validation Loss: 0.24422790342337125
Epoch 177/500, Training Loss: 0.265525741443224
Epoch 177/500, Validation Loss: 0.24653784582367191
Epoch 178/500, Training Loss: 0.2723626709399789
Epoch 178/500, Validation Loss: 0.2507967023880451
Epoch 179/500, Training Loss: 0.2726516236089966
Epoch 179/500, Validation Loss: 0.24386514936174666
Epoch 180/500, Training Loss: 0.2601773423916465
Epoch 180/500, Validation Loss: 0.2542456957426938
Epoch 181/500, Training Loss: 0.2643322729493973
Epoch 181/500, Validation Loss: 0.2438225368787716
Epoch 182/500, Training Loss: 0.2624188568572231
Epoch 182/500, Validation Loss: 0.24593104499501067
Epoch 183/500, Training Loss: 0.26099340784480995
Epoch 183/500, Validation Loss: 0.24647329186464284
Epoch 184/500, Training Loss: 0.26017148517867894
Epoch 184/500, Validation Loss: 0.2392259963921138
Epoch 185/500, Training Loss: 0.26222564600377074
Epoch 185/500, Validation Loss: 0.23977841069171954
Epoch 186/500, Training Loss: 0.2623341355066561
Epoch 186/500, Validation Loss: 0.24207699144041384
Epoch 187/500, Training Loss: 0.25709320303752126
Epoch 187/500, Validation Loss: 0.24145985230222924
Epoch 188/500, Training Loss: 0.25602950739329383
Epoch 188/500, Validation Loss: 0.23893912871936698
Epoch 189/500, Training Loss: 0.25709123762735214
Epoch 189/500, Validation Loss: 0.24104929241267117
Epoch 190/500, Training Loss: 0.253911294812923
Epoch 190/500, Validation Loss: 0.25578864189711487
Epoch 191/500, Training Loss: 0.26093535894210274
Epoch 191/500, Validation Loss: 0.23687874380644267
Epoch 192/500, Training Loss: 0.25595243314756455
Epoch 192/500, Validation Loss: 0.23669037880835594
Epoch 193/500, Training Loss: 0.2526634229029349
Epoch 193/500, Validation Loss: 0.23560003704064852
Epoch 194/500, Training Loss: 0.2535334516332153
Epoch 194/500, Validation Loss: 0.23670798540115356
Epoch 195/500, Training Loss: 0.2516338016331485
Epoch 195/500, Validation Loss: 0.2419825993575059
Epoch 196/500, Training Loss: 0.2556088642009254
Epoch 196/500, Validation Loss: 0.2359942826744798
Epoch 197/500, Training Loss: 0.26068950089498133
Epoch 197/500, Validation Loss: 0.23189087502368086
Epoch 198/500, Training Loss: 0.25339730901336205
Epoch 198/500, Validation Loss: 0.23397363257872594
Epoch 199/500, Training Loss: 0.2502644243914939
Epoch 199/500, Validation Loss: 0.2338287460726577
Epoch 200/500, Training Loss: 0.2567370927242895
Epoch 200/500, Validation Loss: 0.23556989760367902
Epoch 201/500, Training Loss: 0.2496631381298246
Epoch 201/500, Validation Loss: 0.23703868890350516
Epoch 202/500, Training Loss: 0.2496082384278804
Epoch 202/500, Validation Loss: 0.23420529802898307
Epoch 203/500, Training Loss: 0.24782099130456225
Epoch 203/500, Validation Loss: 0.23296853919307908
Epoch 204/500, Training Loss: 0.25048537640386725
Epoch 204/500, Validation Loss: 0.23180617508176085
Epoch 205/500, Training Loss: 0.2501316369716569
Epoch 205/500, Validation Loss: 0.2310430808113767
Epoch 206/500, Training Loss: 0.24574442842300986
Epoch 206/500, Validation Loss: 0.2335695049592427
Epoch 207/500, Training Loss: 0.2540670616782158
Epoch 207/500, Validation Loss: 0.23927457495169205
Epoch 208/500, Training Loss: 0.2532785813454616
Epoch 208/500, Validation Loss: 0.2303126057634106
Epoch 209/500, Training Loss: 0.2720511001993317
Epoch 209/500, Validation Loss: 0.22645220276597258
Epoch 210/500, Training Loss: 0.24657765059892073
Epoch 210/500, Validation Loss: 0.23274229815253963
Epoch 211/500, Training Loss: 0.24701759896025413
Epoch 211/500, Validation Loss: 0.2305621054845971
Epoch 212/500, Training Loss: 0.24744532723561052
Epoch 212/500, Validation Loss: 0.22839013961228458
Epoch 213/500, Training Loss: 0.2452756304516045
Epoch 213/500, Validation Loss: 0.2464297396409047
Epoch 214/500, Training Loss: 0.2650119272911929
Epoch 214/500, Validation Loss: 0.2262352869495169
Epoch 215/500, Training Loss: 0.2467165436704182
Epoch 215/500, Validation Loss: 0.22604420239275153
Epoch 216/500, Training Loss: 0.2420814569714
Epoch 216/500, Validation Loss: 0.2252466030322112
Epoch 217/500, Training Loss: 0.2521569495723581
Epoch 217/500, Validation Loss: 0.2454007538882169
Epoch 218/500, Training Loss: 0.2556491313132764
Epoch 218/500, Validation Loss: 0.22758814044199982
Epoch 219/500, Training Loss: 0.24565299936207258
Epoch 219/500, Validation Loss: 0.2249795932854925
Epoch 220/500, Training Loss: 0.2430572368607607
Epoch 220/500, Validation Loss: 0.22574804965164755
Epoch 221/500, Training Loss: 0.24265857526564896
Epoch 221/500, Validation Loss: 0.23588964710761975
Epoch 222/500, Training Loss: 0.24561873644631713
Epoch 222/500, Validation Loss: 0.2301856598683766
Epoch 223/500, Training Loss: 0.2405612554282322
Epoch 223/500, Validation Loss: 0.22812038901951406
Epoch 224/500, Training Loss: 0.24113539791305585
Epoch 224/500, Validation Loss: 0.22799332517308074
Epoch 225/500, Training Loss: 0.23886136064013974
Epoch 225/500, Validation Loss: 0.22731984667963795
Epoch 226/500, Training Loss: 0.24064113487580427
Epoch 226/500, Validation Loss: 0.23651765054696566
Epoch 227/500, Training Loss: 0.24332036406583726
Epoch 227/500, Validation Loss: 0.22834710709073328
Epoch 228/500, Training Loss: 0.24345629979519176
Epoch 228/500, Validation Loss: 0.23118927097552783
Epoch 229/500, Training Loss: 0.23927709519642812
Epoch 229/500, Validation Loss: 0.2297859786973371
Early stopping triggered
Classification Accuracy: 0.9161290322580645
Classification Report:
              precision    recall  f1-score   support

           0       0.95      0.90      0.93        93
           1       0.87      0.94      0.90        62

    accuracy                           0.92       155
   macro avg       0.91      0.92      0.91       155
weighted avg       0.92      0.92      0.92       155

graph(%0 : Float(8, strides=[1], requires_grad=0, device=cpu),
      %fc1.bias : Float(24, strides=[1], requires_grad=1, device=cpu),
      %fc2.bias : Float(16, strides=[1], requires_grad=1, device=cpu),
      %fc3.bias : Float(8, strides=[1], requires_grad=1, device=cpu),
      %fc4.bias : Float(2, strides=[1], requires_grad=1, device=cpu),
      %24 : Float(8, 24, strides=[1, 8], requires_grad=0, device=cpu),
      %25 : Float(24, 16, strides=[1, 24], requires_grad=0, device=cpu),
      %26 : Float(16, 8, strides=[1, 16], requires_grad=0, device=cpu),
      %27 : Float(8, 2, strides=[1, 8], requires_grad=0, device=cpu)):
  %10 : Float(24, strides=[1], device=cpu) = onnx::MatMul(%0, %24) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  %11 : Float(24, strides=[1], requires_grad=1, device=cpu) = onnx::Add(%fc1.bias, %10) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  %12 : Float(24, strides=[1], requires_grad=1, device=cpu) = onnx::Relu(%11) # /home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py:67:0
  %14 : Float(16, strides=[1], device=cpu) = onnx::MatMul(%12, %25) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  %15 : Float(16, strides=[1], requires_grad=1, device=cpu) = onnx::Add(%fc2.bias, %14) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  %16 : Float(16, strides=[1], requires_grad=1, device=cpu) = onnx::Relu(%15) # /home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py:68:0
  %18 : Float(8, strides=[1], device=cpu) = onnx::MatMul(%16, %26) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  %19 : Float(8, strides=[1], requires_grad=1, device=cpu) = onnx::Add(%fc3.bias, %18) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  %20 : Float(8, strides=[1], requires_grad=1, device=cpu) = onnx::Relu(%19) # /home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py:69:0
  %22 : Float(2, strides=[1], device=cpu) = onnx::MatMul(%20, %27) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  %23 : Float(2, strides=[1], requires_grad=1, device=cpu) = onnx::Add(%fc4.bias, %22) # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103:0
  return (%23)

Model has been exported as datasets/concrete/split/concrete.onnx

Process finished with exit code 0
