/usr/bin/python3.10 /home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py
/home/mustafa/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
Epoch 1/500, Training Loss: 0.7159949211240972
Epoch 1/500, Validation Loss: 0.7024252554038902
Epoch 2/500, Training Loss: 0.7016864778595394
Epoch 2/500, Validation Loss: 0.6897727042049556
Epoch 3/500, Training Loss: 0.6904918539044596
Epoch 3/500, Validation Loss: 0.6796525902562327
Epoch 4/500, Training Loss: 0.6822902754507184
Epoch 4/500, Validation Loss: 0.6733812449814437
Epoch 5/500, Training Loss: 0.676029111079138
Epoch 5/500, Validation Loss: 0.6672269446509225
Epoch 6/500, Training Loss: 0.6698324343037175
Epoch 6/500, Validation Loss: 0.6612433349931395
Epoch 7/500, Training Loss: 0.6646459177859778
Epoch 7/500, Validation Loss: 0.6562303436266912
Epoch 8/500, Training Loss: 0.6600945792813242
Epoch 8/500, Validation Loss: 0.6521569931661928
Epoch 9/500, Training Loss: 0.6562904836235364
Epoch 9/500, Validation Loss: 0.6487051325959045
Epoch 10/500, Training Loss: 0.6528139443404135
Epoch 10/500, Validation Loss: 0.6452601188189023
Epoch 11/500, Training Loss: 0.6491473503979168
Epoch 11/500, Validation Loss: 0.6417110492656757
Epoch 12/500, Training Loss: 0.645992847652938
Epoch 12/500, Validation Loss: 0.6386161895541401
Epoch 13/500, Training Loss: 0.6428603425766658
Epoch 13/500, Validation Loss: 0.6355150597436088
Epoch 14/500, Training Loss: 0.6400041733138604
Epoch 14/500, Validation Loss: 0.6324123066741151
Epoch 15/500, Training Loss: 0.6367079978511667
Epoch 15/500, Validation Loss: 0.6295618399397119
Epoch 16/500, Training Loss: 0.6338540207331125
Epoch 16/500, Validation Loss: 0.6267372005945676
Epoch 17/500, Training Loss: 0.6313324479719471
Epoch 17/500, Validation Loss: 0.6241123800153856
Epoch 18/500, Training Loss: 0.6286767703402224
Epoch 18/500, Validation Loss: 0.6213936666389565
Epoch 19/500, Training Loss: 0.6259422839597261
Epoch 19/500, Validation Loss: 0.6187273952868078
Epoch 20/500, Training Loss: 0.6234530101371374
Epoch 20/500, Validation Loss: 0.6162434289981793
Epoch 21/500, Training Loss: 0.6210975222878582
Epoch 21/500, Validation Loss: 0.6137931672009554
Epoch 22/500, Training Loss: 0.6186936714282149
Epoch 22/500, Validation Loss: 0.6111647958879347
Epoch 23/500, Training Loss: 0.6162741650490093
Epoch 23/500, Validation Loss: 0.608686427017311
Epoch 24/500, Training Loss: 0.613796403933828
Epoch 24/500, Validation Loss: 0.6062012577985788
Epoch 25/500, Training Loss: 0.6116377664174518
Epoch 25/500, Validation Loss: 0.6038144912038531
Epoch 26/500, Training Loss: 0.6091431044507126
Epoch 26/500, Validation Loss: 0.6014713002489759
Epoch 27/500, Training Loss: 0.6068748259676644
Epoch 27/500, Validation Loss: 0.599120062667054
Epoch 28/500, Training Loss: 0.604714560235919
Epoch 28/500, Validation Loss: 0.596855666730311
Epoch 29/500, Training Loss: 0.6024501102972626
Epoch 29/500, Validation Loss: 0.5946123429707119
Epoch 30/500, Training Loss: 0.600316807276333
Epoch 30/500, Validation Loss: 0.5925116864117709
Epoch 31/500, Training Loss: 0.5982241261435283
Epoch 31/500, Validation Loss: 0.5903216352710476
Epoch 32/500, Training Loss: 0.5961550462593153
Epoch 32/500, Validation Loss: 0.5883173400705511
Epoch 33/500, Training Loss: 0.5942299480907794
Epoch 33/500, Validation Loss: 0.5861992332842443
Epoch 34/500, Training Loss: 0.5922169597734195
Epoch 34/500, Validation Loss: 0.584177946115469
Epoch 35/500, Training Loss: 0.5902648287904081
Epoch 35/500, Validation Loss: 0.5821583611624581
Epoch 36/500, Training Loss: 0.5884534605002436
Epoch 36/500, Validation Loss: 0.5802057501557586
Epoch 37/500, Training Loss: 0.5866726901428709
Epoch 37/500, Validation Loss: 0.5783005666423153
Epoch 38/500, Training Loss: 0.5848788107317794
Epoch 38/500, Validation Loss: 0.5764215147340452
Epoch 39/500, Training Loss: 0.5832439695333806
Epoch 39/500, Validation Loss: 0.574712487784299
Epoch 40/500, Training Loss: 0.5813998381572358
Epoch 40/500, Validation Loss: 0.5727549566851033
Epoch 41/500, Training Loss: 0.5795488977564522
Epoch 41/500, Validation Loss: 0.5708151420989593
Epoch 42/500, Training Loss: 0.5778240316370489
Epoch 42/500, Validation Loss: 0.5690952400108437
Epoch 43/500, Training Loss: 0.5762384545455859
Epoch 43/500, Validation Loss: 0.5673397516275381
Epoch 44/500, Training Loss: 0.5745441749919304
Epoch 44/500, Validation Loss: 0.5655831446895352
Epoch 45/500, Training Loss: 0.572911366096978
Epoch 45/500, Validation Loss: 0.5638114686136122
Epoch 46/500, Training Loss: 0.5712548510046177
Epoch 46/500, Validation Loss: 0.5621033171554665
Epoch 47/500, Training Loss: 0.5695928590638297
Epoch 47/500, Validation Loss: 0.5604794362148682
Epoch 48/500, Training Loss: 0.5680851844577287
Epoch 48/500, Validation Loss: 0.5587099372566521
Epoch 49/500, Training Loss: 0.5665872391318482
Epoch 49/500, Validation Loss: 0.5571501785284513
Epoch 50/500, Training Loss: 0.5649414737909082
Epoch 50/500, Validation Loss: 0.5554406205555061
Epoch 51/500, Training Loss: 0.5635116435535737
Epoch 51/500, Validation Loss: 0.5537794723913267
Epoch 52/500, Training Loss: 0.5620663032121698
Epoch 52/500, Validation Loss: 0.5521110691033401
Epoch 53/500, Training Loss: 0.5606156666963342
Epoch 53/500, Validation Loss: 0.550495191827997
Epoch 54/500, Training Loss: 0.5591375219342447
Epoch 54/500, Validation Loss: 0.5489023420717809
Epoch 55/500, Training Loss: 0.5577885496136881
Epoch 55/500, Validation Loss: 0.5473832147462028
Epoch 56/500, Training Loss: 0.5566807612466746
Epoch 56/500, Validation Loss: 0.5459342660842004
Epoch 57/500, Training Loss: 0.5551135783056611
Epoch 57/500, Validation Loss: 0.5446056487498345
Epoch 58/500, Training Loss: 0.5537564324191804
Epoch 58/500, Validation Loss: 0.5432150046546738
Epoch 59/500, Training Loss: 0.5523990146330094
Epoch 59/500, Validation Loss: 0.5417814815973306
Epoch 60/500, Training Loss: 0.5513297610408555
Epoch 60/500, Validation Loss: 0.5407079136990881
Epoch 61/500, Training Loss: 0.5502012116568429
Epoch 61/500, Validation Loss: 0.5393192682947431
Epoch 62/500, Training Loss: 0.5488933301666408
Epoch 62/500, Validation Loss: 0.5381287065419283
Epoch 63/500, Training Loss: 0.5479079227424363
Epoch 63/500, Validation Loss: 0.5369891067603966
Epoch 64/500, Training Loss: 0.5465334335990488
Epoch 64/500, Validation Loss: 0.5356413691074817
Epoch 65/500, Training Loss: 0.5453842709282731
Epoch 65/500, Validation Loss: 0.5342779620127245
Epoch 66/500, Training Loss: 0.544096052646637
Epoch 66/500, Validation Loss: 0.5329132707088025
Epoch 67/500, Training Loss: 0.5428356534069024
Epoch 67/500, Validation Loss: 0.5316666434337567
Epoch 68/500, Training Loss: 0.5417954648086665
Epoch 68/500, Validation Loss: 0.5304114915333785
Epoch 69/500, Training Loss: 0.5406294233590654
Epoch 69/500, Validation Loss: 0.5290920417030136
Epoch 70/500, Training Loss: 0.5395186579012508
Epoch 70/500, Validation Loss: 0.5278452162618761
Epoch 71/500, Training Loss: 0.5385051705718206
Epoch 71/500, Validation Loss: 0.5266673572651752
Epoch 72/500, Training Loss: 0.5372802452306973
Epoch 72/500, Validation Loss: 0.5253453568204657
Epoch 73/500, Training Loss: 0.5362025758099126
Epoch 73/500, Validation Loss: 0.5241132301169557
Epoch 74/500, Training Loss: 0.5352750382542445
Epoch 74/500, Validation Loss: 0.5228457203159085
Epoch 75/500, Training Loss: 0.5342435928555037
Epoch 75/500, Validation Loss: 0.5218543661879255
Epoch 76/500, Training Loss: 0.5332347985444882
Epoch 76/500, Validation Loss: 0.5206966032455493
Epoch 77/500, Training Loss: 0.5322896872909324
Epoch 77/500, Validation Loss: 0.5195634090281153
Epoch 78/500, Training Loss: 0.5313511954183883
Epoch 78/500, Validation Loss: 0.518506232407186
Epoch 79/500, Training Loss: 0.5302131013962829
Epoch 79/500, Validation Loss: 0.5174463190041579
Epoch 80/500, Training Loss: 0.5292127220458693
Epoch 80/500, Validation Loss: 0.5162957898982159
Epoch 81/500, Training Loss: 0.5282448981572123
Epoch 81/500, Validation Loss: 0.5151263270285222
Epoch 82/500, Training Loss: 0.5274410961239745
Epoch 82/500, Validation Loss: 0.5143562903651944
Epoch 83/500, Training Loss: 0.5266608684304352
Epoch 83/500, Validation Loss: 0.5132397374549469
Epoch 84/500, Training Loss: 0.5255936378827538
Epoch 84/500, Validation Loss: 0.5122031147603865
Epoch 85/500, Training Loss: 0.524962660798418
Epoch 85/500, Validation Loss: 0.511085101149299
Epoch 86/500, Training Loss: 0.5238965670377966
Epoch 86/500, Validation Loss: 0.5099881837120304
Epoch 87/500, Training Loss: 0.5230006927756099
Epoch 87/500, Validation Loss: 0.5091495672603706
Epoch 88/500, Training Loss: 0.5220670979495848
Epoch 88/500, Validation Loss: 0.5081208220549992
Epoch 89/500, Training Loss: 0.5212209125497635
Epoch 89/500, Validation Loss: 0.5071988361222404
Epoch 90/500, Training Loss: 0.5204037265043484
Epoch 90/500, Validation Loss: 0.5061865565064666
Epoch 91/500, Training Loss: 0.5195583084781606
Epoch 91/500, Validation Loss: 0.5052588825876062
Epoch 92/500, Training Loss: 0.5188114001995647
Epoch 92/500, Validation Loss: 0.5042937235398726
Epoch 93/500, Training Loss: 0.5178877143357233
Epoch 93/500, Validation Loss: 0.5033452634687547
Epoch 94/500, Training Loss: 0.5170470078510649
Epoch 94/500, Validation Loss: 0.5023940081720228
Epoch 95/500, Training Loss: 0.5161887039423981
Epoch 95/500, Validation Loss: 0.5014811889691786
Epoch 96/500, Training Loss: 0.515513013536821
Epoch 96/500, Validation Loss: 0.5004520005994029
Epoch 97/500, Training Loss: 0.5146607894259253
Epoch 97/500, Validation Loss: 0.4994689740917899
Epoch 98/500, Training Loss: 0.5137739359257787
Epoch 98/500, Validation Loss: 0.49855532042391887
Epoch 99/500, Training Loss: 0.513122747410683
Epoch 99/500, Validation Loss: 0.4977066048554012
Epoch 100/500, Training Loss: 0.5122428502520642
Epoch 100/500, Validation Loss: 0.4970391323040058
Epoch 101/500, Training Loss: 0.5116005320820167
Epoch 101/500, Validation Loss: 0.49601737554971276
Epoch 102/500, Training Loss: 0.5107854942838957
Epoch 102/500, Validation Loss: 0.49497427955850376
Epoch 103/500, Training Loss: 0.5101339836887777
Epoch 103/500, Validation Loss: 0.4940344300362971
Epoch 104/500, Training Loss: 0.509273066682723
Epoch 104/500, Validation Loss: 0.49326139378857303
Epoch 105/500, Training Loss: 0.5085977122042943
Epoch 105/500, Validation Loss: 0.4923824679541898
Epoch 106/500, Training Loss: 0.5079300877290691
Epoch 106/500, Validation Loss: 0.4914390920818626
Epoch 107/500, Training Loss: 0.5071205723302209
Epoch 107/500, Validation Loss: 0.49053689876160067
Epoch 108/500, Training Loss: 0.5063611143133016
Epoch 108/500, Validation Loss: 0.4897853716627344
Epoch 109/500, Training Loss: 0.5057574039358702
Epoch 109/500, Validation Loss: 0.48902085229947967
Epoch 110/500, Training Loss: 0.5051393996660654
Epoch 110/500, Validation Loss: 0.4883657589361265
Epoch 111/500, Training Loss: 0.5045080534628129
Epoch 111/500, Validation Loss: 0.4874869886930887
Epoch 112/500, Training Loss: 0.5038430294778906
Epoch 112/500, Validation Loss: 0.48675635805377715
Epoch 113/500, Training Loss: 0.5031553215028178
Epoch 113/500, Validation Loss: 0.4859044002248095
Epoch 114/500, Training Loss: 0.5024376851758414
Epoch 114/500, Validation Loss: 0.4849620894952254
Epoch 115/500, Training Loss: 0.5017557784026274
Epoch 115/500, Validation Loss: 0.48417705103948516
Epoch 116/500, Training Loss: 0.5011508681407087
Epoch 116/500, Validation Loss: 0.4834931241227435
Epoch 117/500, Training Loss: 0.5005413685691505
Epoch 117/500, Validation Loss: 0.4827395336968558
Epoch 118/500, Training Loss: 0.4999017458765583
Epoch 118/500, Validation Loss: 0.48186846251611587
Epoch 119/500, Training Loss: 0.4991699868862242
Epoch 119/500, Validation Loss: 0.48118142570768085
Epoch 120/500, Training Loss: 0.49862020794462397
Epoch 120/500, Validation Loss: 0.4805191314065611
Epoch 121/500, Training Loss: 0.4980242822768784
Epoch 121/500, Validation Loss: 0.47989772124723956
Epoch 122/500, Training Loss: 0.49735965510512525
Epoch 122/500, Validation Loss: 0.47901656410910864
Epoch 123/500, Training Loss: 0.4968334356640645
Epoch 123/500, Validation Loss: 0.47839091002167045
Epoch 124/500, Training Loss: 0.4962683358900094
Epoch 124/500, Validation Loss: 0.4778148288076574
Epoch 125/500, Training Loss: 0.4957440177213798
Epoch 125/500, Validation Loss: 0.4770438682723355
Epoch 126/500, Training Loss: 0.49522882480892494
Epoch 126/500, Validation Loss: 0.4763002461427218
Epoch 127/500, Training Loss: 0.49449292151838664
Epoch 127/500, Validation Loss: 0.47575753849822205
Epoch 128/500, Training Loss: 0.493942560501469
Epoch 128/500, Validation Loss: 0.4749514940497163
Epoch 129/500, Training Loss: 0.49338531018791515
Epoch 129/500, Validation Loss: 0.47421456350908653
Epoch 130/500, Training Loss: 0.4928541340543559
Epoch 130/500, Validation Loss: 0.4734973172088722
Epoch 131/500, Training Loss: 0.4922625874679396
Epoch 131/500, Validation Loss: 0.47284126978415947
Epoch 132/500, Training Loss: 0.4918450833028299
Epoch 132/500, Validation Loss: 0.47233086785712797
Epoch 133/500, Training Loss: 0.4913376769807898
Epoch 133/500, Validation Loss: 0.4715273252555302
Epoch 134/500, Training Loss: 0.49072431655660087
Epoch 134/500, Validation Loss: 0.47097884641065224
Epoch 135/500, Training Loss: 0.4902588836280384
Epoch 135/500, Validation Loss: 0.4703040618401069
Epoch 136/500, Training Loss: 0.48968830927896434
Epoch 136/500, Validation Loss: 0.4695353430586976
Epoch 137/500, Training Loss: 0.48909302665522625
Epoch 137/500, Validation Loss: 0.4690067059807963
Epoch 138/500, Training Loss: 0.4886159374379913
Epoch 138/500, Validation Loss: 0.4683241139758717
Epoch 139/500, Training Loss: 0.48811748813987604
Epoch 139/500, Validation Loss: 0.4677173812668045
Epoch 140/500, Training Loss: 0.4875939688264249
Epoch 140/500, Validation Loss: 0.4670590533838644
Epoch 141/500, Training Loss: 0.48716178978696445
Epoch 141/500, Validation Loss: 0.4665061444431156
Epoch 142/500, Training Loss: 0.48655549278702387
Epoch 142/500, Validation Loss: 0.4657940709745729
Epoch 143/500, Training Loss: 0.48615754229351155
Epoch 143/500, Validation Loss: 0.4652448966131582
Epoch 144/500, Training Loss: 0.4855381989445997
Epoch 144/500, Validation Loss: 0.4645927358757366
Epoch 145/500, Training Loss: 0.48509412995654566
Epoch 145/500, Validation Loss: 0.4639764826793175
Epoch 146/500, Training Loss: 0.4846829504766676
Epoch 146/500, Validation Loss: 0.46340952949090436
Epoch 147/500, Training Loss: 0.48426334867861004
Epoch 147/500, Validation Loss: 0.46290384291054365
Epoch 148/500, Training Loss: 0.4837992059043641
Epoch 148/500, Validation Loss: 0.4624572264683711
Epoch 149/500, Training Loss: 0.4833302052275648
Epoch 149/500, Validation Loss: 0.46187999031760474
Epoch 150/500, Training Loss: 0.48292706970864296
Epoch 150/500, Validation Loss: 0.4613117838834787
Epoch 151/500, Training Loss: 0.48247496827134806
Epoch 151/500, Validation Loss: 0.4608039085741167
Epoch 152/500, Training Loss: 0.48202278147788713
Epoch 152/500, Validation Loss: 0.46022076227448205
Epoch 153/500, Training Loss: 0.4816313106127486
Epoch 153/500, Validation Loss: 0.4596483242976201
Epoch 154/500, Training Loss: 0.48117729203049586
Epoch 154/500, Validation Loss: 0.45915737090172704
Epoch 155/500, Training Loss: 0.4808067561187956
Epoch 155/500, Validation Loss: 0.45869991338098204
Epoch 156/500, Training Loss: 0.48045754331408197
Epoch 156/500, Validation Loss: 0.4582578365678911
Epoch 157/500, Training Loss: 0.47996809776877564
Epoch 157/500, Validation Loss: 0.457698541028159
Epoch 158/500, Training Loss: 0.47954734304245566
Epoch 158/500, Validation Loss: 0.4570957534498983
Epoch 159/500, Training Loss: 0.47906911499209276
Epoch 159/500, Validation Loss: 0.4564958733397645
Epoch 160/500, Training Loss: 0.47864020482511693
Epoch 160/500, Validation Loss: 0.4559375624378006
Epoch 161/500, Training Loss: 0.4784253645538457
Epoch 161/500, Validation Loss: 0.4554007510086159
Epoch 162/500, Training Loss: 0.47802162988837316
Epoch 162/500, Validation Loss: 0.4549921871005715
Epoch 163/500, Training Loss: 0.47751792959465894
Epoch 163/500, Validation Loss: 0.4544592991277769
Epoch 164/500, Training Loss: 0.4771572451485675
Epoch 164/500, Validation Loss: 0.453984140188663
Epoch 165/500, Training Loss: 0.47723373679777453
Epoch 165/500, Validation Loss: 0.45361662723801355
Epoch 166/500, Training Loss: 0.4764162601280477
Epoch 166/500, Validation Loss: 0.45309753580526874
Epoch 167/500, Training Loss: 0.47598878284929197
Epoch 167/500, Validation Loss: 0.45283607229009853
Epoch 168/500, Training Loss: 0.47570970305209354
Epoch 168/500, Validation Loss: 0.45229927866489855
Epoch 169/500, Training Loss: 0.47524374463025804
Epoch 169/500, Validation Loss: 0.45179711379014054
Epoch 170/500, Training Loss: 0.4748503191295841
Epoch 170/500, Validation Loss: 0.45127034109908265
Epoch 171/500, Training Loss: 0.47453283759327436
Epoch 171/500, Validation Loss: 0.45073398212333776
Epoch 172/500, Training Loss: 0.4742573366995162
Epoch 172/500, Validation Loss: 0.450284732239587
Epoch 173/500, Training Loss: 0.4737749589134354
Epoch 173/500, Validation Loss: 0.4497930651361292
Epoch 174/500, Training Loss: 0.47350268590105143
Epoch 174/500, Validation Loss: 0.4492894403346173
Epoch 175/500, Training Loss: 0.4731430197225034
Epoch 175/500, Validation Loss: 0.44892165103516024
Epoch 176/500, Training Loss: 0.4727936342833938
Epoch 176/500, Validation Loss: 0.4483380097073394
Epoch 177/500, Training Loss: 0.47249382600994283
Epoch 177/500, Validation Loss: 0.44799427900995525
Epoch 178/500, Training Loss: 0.47211960743435877
Epoch 178/500, Validation Loss: 0.4475780311342958
Epoch 179/500, Training Loss: 0.4717161212317987
Epoch 179/500, Validation Loss: 0.44706027306519547
Epoch 180/500, Training Loss: 0.4714798067351506
Epoch 180/500, Validation Loss: 0.44673652501849387
Epoch 181/500, Training Loss: 0.4709972795848211
Epoch 181/500, Validation Loss: 0.4462998199772525
Epoch 182/500, Training Loss: 0.4706915204055432
Epoch 182/500, Validation Loss: 0.44588679191354036
Epoch 183/500, Training Loss: 0.4703273445409148
Epoch 183/500, Validation Loss: 0.44541473860864517
Epoch 184/500, Training Loss: 0.4700316556118398
Epoch 184/500, Validation Loss: 0.44494785271681747
Epoch 185/500, Training Loss: 0.4696429506998287
Epoch 185/500, Validation Loss: 0.4445442685059139
Epoch 186/500, Training Loss: 0.4693580120181905
Epoch 186/500, Validation Loss: 0.44413351083730723
Epoch 187/500, Training Loss: 0.4692409019943084
Epoch 187/500, Validation Loss: 0.4436019960936014
Epoch 188/500, Training Loss: 0.46895525888662565
Epoch 188/500, Validation Loss: 0.4432193079552093
Epoch 189/500, Training Loss: 0.46859893917872075
Epoch 189/500, Validation Loss: 0.4428730970853335
Epoch 190/500, Training Loss: 0.4682511585595373
Epoch 190/500, Validation Loss: 0.44261061990415895
Epoch 191/500, Training Loss: 0.46789418016243906
Epoch 191/500, Validation Loss: 0.442270263061895
Epoch 192/500, Training Loss: 0.4676197867287677
Epoch 192/500, Validation Loss: 0.4417788575996052
Epoch 193/500, Training Loss: 0.4673558471520301
Epoch 193/500, Validation Loss: 0.44148396903818304
Epoch 194/500, Training Loss: 0.46699437699519647
Epoch 194/500, Validation Loss: 0.4410803407043606
Epoch 195/500, Training Loss: 0.46672495054636515
Epoch 195/500, Validation Loss: 0.44069438243841197
Epoch 196/500, Training Loss: 0.46640089348516584
Epoch 196/500, Validation Loss: 0.44050003297917256
Epoch 197/500, Training Loss: 0.46616442619348863
Epoch 197/500, Validation Loss: 0.4401859323699753
Epoch 198/500, Training Loss: 0.465980663161916
Epoch 198/500, Validation Loss: 0.4398955207366448
Epoch 199/500, Training Loss: 0.465641244274568
Epoch 199/500, Validation Loss: 0.43950569823190766
Epoch 200/500, Training Loss: 0.4653465518348136
Epoch 200/500, Validation Loss: 0.439083265793788
Epoch 201/500, Training Loss: 0.46509542966186584
Epoch 201/500, Validation Loss: 0.43877115032889624
Epoch 202/500, Training Loss: 0.4650122230244741
Epoch 202/500, Validation Loss: 0.43856007867045216
Epoch 203/500, Training Loss: 0.46460516085840464
Epoch 203/500, Validation Loss: 0.438039853201284
Epoch 204/500, Training Loss: 0.4642999659340193
Epoch 204/500, Validation Loss: 0.4376737254780608
Epoch 205/500, Training Loss: 0.4640514728265728
Epoch 205/500, Validation Loss: 0.43732902872097956
Epoch 206/500, Training Loss: 0.4637536108493805
Epoch 206/500, Validation Loss: 0.4370676700945024
Epoch 207/500, Training Loss: 0.46346898209536125
Epoch 207/500, Validation Loss: 0.43672624385202086
Epoch 208/500, Training Loss: 0.4632471800677158
Epoch 208/500, Validation Loss: 0.43638481490023723
Epoch 209/500, Training Loss: 0.46298983645587954
Epoch 209/500, Validation Loss: 0.4360410369835891
Epoch 210/500, Training Loss: 0.46268922578279914
Epoch 210/500, Validation Loss: 0.4356800186169612
Epoch 211/500, Training Loss: 0.4625071667227765
Epoch 211/500, Validation Loss: 0.4354816381033365
Epoch 212/500, Training Loss: 0.46235188457407006
Epoch 212/500, Validation Loss: 0.4351434065149976
Epoch 213/500, Training Loss: 0.4620233991812403
Epoch 213/500, Validation Loss: 0.43481785562131314
Epoch 214/500, Training Loss: 0.46188040736974856
Epoch 214/500, Validation Loss: 0.43444768716762594
Epoch 215/500, Training Loss: 0.4616341302365106
Epoch 215/500, Validation Loss: 0.43415526987670305
Epoch 216/500, Training Loss: 0.4613196269483738
Epoch 216/500, Validation Loss: 0.43381505894970585
Epoch 217/500, Training Loss: 0.46111838331500304
Epoch 217/500, Validation Loss: 0.43343336280290184
Epoch 218/500, Training Loss: 0.460847061773279
Epoch 218/500, Validation Loss: 0.4331889206712896
Epoch 219/500, Training Loss: 0.46072168724712154
Epoch 219/500, Validation Loss: 0.4328577115938261
Epoch 220/500, Training Loss: 0.46035719190655733
Epoch 220/500, Validation Loss: 0.4325266975861091
Epoch 221/500, Training Loss: 0.4600887697050542
Epoch 221/500, Validation Loss: 0.43226498055767704
Epoch 222/500, Training Loss: 0.45974118967286093
Epoch 222/500, Validation Loss: 0.43209865031304295
Epoch 223/500, Training Loss: 0.45959552444631285
Epoch 223/500, Validation Loss: 0.43184589952617497
Epoch 224/500, Training Loss: 0.45937239981144046
Epoch 224/500, Validation Loss: 0.43142538295163735
Epoch 225/500, Training Loss: 0.4591606457752593
Epoch 225/500, Validation Loss: 0.431086775931445
Epoch 226/500, Training Loss: 0.45886704247140025
Epoch 226/500, Validation Loss: 0.4308359061742758
Epoch 227/500, Training Loss: 0.4586260167471083
Epoch 227/500, Validation Loss: 0.43056534404878494
Epoch 228/500, Training Loss: 0.4583882687409278
Epoch 228/500, Validation Loss: 0.43025086104095756
Epoch 229/500, Training Loss: 0.4581609825692792
Epoch 229/500, Validation Loss: 0.42991881556325146
Epoch 230/500, Training Loss: 0.4579927115268416
Epoch 230/500, Validation Loss: 0.429632295648773
Epoch 231/500, Training Loss: 0.4577257321132205
Epoch 231/500, Validation Loss: 0.42914306033741345
Epoch 232/500, Training Loss: 0.45749392121740917
Epoch 232/500, Validation Loss: 0.42889828883208236
Epoch 233/500, Training Loss: 0.4573416544561082
Epoch 233/500, Validation Loss: 0.42865266350956704
Epoch 234/500, Training Loss: 0.4571095259444227
Epoch 234/500, Validation Loss: 0.4283624380439907
Epoch 235/500, Training Loss: 0.4568948408022998
Epoch 235/500, Validation Loss: 0.42802184239610447
Epoch 236/500, Training Loss: 0.4566859280930809
Epoch 236/500, Validation Loss: 0.42772479645617595
Epoch 237/500, Training Loss: 0.45650046939823397
Epoch 237/500, Validation Loss: 0.42740496567317415
Epoch 238/500, Training Loss: 0.45625741589664587
Epoch 238/500, Validation Loss: 0.4270963842992659
Epoch 239/500, Training Loss: 0.456018477388956
Epoch 239/500, Validation Loss: 0.42675767548672566
Epoch 240/500, Training Loss: 0.45564179531661547
Epoch 240/500, Validation Loss: 0.42651237605454084
Epoch 241/500, Training Loss: 0.4554681640722881
Epoch 241/500, Validation Loss: 0.4262630358918921
Epoch 242/500, Training Loss: 0.4552426033972371
Epoch 242/500, Validation Loss: 0.42604137047544705
Epoch 243/500, Training Loss: 0.45508489107788025
Epoch 243/500, Validation Loss: 0.4257403751472374
Epoch 244/500, Training Loss: 0.4548070606428112
Epoch 244/500, Validation Loss: 0.4253813349581384
Epoch 245/500, Training Loss: 0.4546344299256884
Epoch 245/500, Validation Loss: 0.4249666224826466
Epoch 246/500, Training Loss: 0.45453855597857135
Epoch 246/500, Validation Loss: 0.42466057043570976
Epoch 247/500, Training Loss: 0.45432591810305806
Epoch 247/500, Validation Loss: 0.42447506220309766
Epoch 248/500, Training Loss: 0.45418035488022845
Epoch 248/500, Validation Loss: 0.42438031017006217
Epoch 249/500, Training Loss: 0.4539362244649006
Epoch 249/500, Validation Loss: 0.4241870366907739
Epoch 250/500, Training Loss: 0.4536632065468787
Epoch 250/500, Validation Loss: 0.4238806026322501
Epoch 251/500, Training Loss: 0.45371980487721636
Epoch 251/500, Validation Loss: 0.42376852306452667
Epoch 252/500, Training Loss: 0.45350206423235667
Epoch 252/500, Validation Loss: 0.4236315693948176
Epoch 253/500, Training Loss: 0.45314770496234646
Epoch 253/500, Validation Loss: 0.42328767613931134
Epoch 254/500, Training Loss: 0.45294203238249153
Epoch 254/500, Validation Loss: 0.4229555137745746
Epoch 255/500, Training Loss: 0.45273364567806257
Epoch 255/500, Validation Loss: 0.42250728684586364
Epoch 256/500, Training Loss: 0.452604420844791
Epoch 256/500, Validation Loss: 0.42230772855993987
Epoch 257/500, Training Loss: 0.45239975556586554
Epoch 257/500, Validation Loss: 0.42211726849729364
Epoch 258/500, Training Loss: 0.45225577182478777
Epoch 258/500, Validation Loss: 0.4219254143052287
Epoch 259/500, Training Loss: 0.45203169969513746
Epoch 259/500, Validation Loss: 0.4216948670226258
Epoch 260/500, Training Loss: 0.45191335229619034
Epoch 260/500, Validation Loss: 0.4217086840759624
Epoch 261/500, Training Loss: 0.4517492298114316
Epoch 261/500, Validation Loss: 0.42136414174909714
Epoch 262/500, Training Loss: 0.4516136879398489
Epoch 262/500, Validation Loss: 0.42097610893187587
Epoch 263/500, Training Loss: 0.45129125620058935
Epoch 263/500, Validation Loss: 0.42074099686238675
Epoch 264/500, Training Loss: 0.4510602572837253
Epoch 264/500, Validation Loss: 0.42028910153872007
Epoch 265/500, Training Loss: 0.45086885968755397
Epoch 265/500, Validation Loss: 0.4199601276354356
Epoch 266/500, Training Loss: 0.4507905482501825
Epoch 266/500, Validation Loss: 0.419764202136498
Epoch 267/500, Training Loss: 0.45051424488445924
Epoch 267/500, Validation Loss: 0.41954570079778697
Epoch 268/500, Training Loss: 0.45040777842314
Epoch 268/500, Validation Loss: 0.41932571747086267
Epoch 269/500, Training Loss: 0.4503306960183275
Epoch 269/500, Validation Loss: 0.4191597359521048
Epoch 270/500, Training Loss: 0.4500827094883595
Epoch 270/500, Validation Loss: 0.4189704857863389
Epoch 271/500, Training Loss: 0.4499155631632481
Epoch 271/500, Validation Loss: 0.4187666340307756
Epoch 272/500, Training Loss: 0.44966056862337084
Epoch 272/500, Validation Loss: 0.4183865770117029
Epoch 273/500, Training Loss: 0.44948488048144747
Epoch 273/500, Validation Loss: 0.41814071714103995
Epoch 274/500, Training Loss: 0.4493541666274263
Epoch 274/500, Validation Loss: 0.41794128696639815
Epoch 275/500, Training Loss: 0.4491751143571904
Epoch 275/500, Validation Loss: 0.41764774376695807
Epoch 276/500, Training Loss: 0.4490845338150797
Epoch 276/500, Validation Loss: 0.41730742137153426
Epoch 277/500, Training Loss: 0.4490536844846114
Epoch 277/500, Validation Loss: 0.41727290215430324
Epoch 278/500, Training Loss: 0.44873472999641867
Epoch 278/500, Validation Loss: 0.4170532079486104
Epoch 279/500, Training Loss: 0.44854815637023704
Epoch 279/500, Validation Loss: 0.41674478177900437
Epoch 280/500, Training Loss: 0.44840489718520526
Epoch 280/500, Validation Loss: 0.41652096517674336
Epoch 281/500, Training Loss: 0.44822222246541726
Epoch 281/500, Validation Loss: 0.4161417921642204
Epoch 282/500, Training Loss: 0.44799108648267105
Epoch 282/500, Validation Loss: 0.41594756655878834
Epoch 283/500, Training Loss: 0.44786339245953605
Epoch 283/500, Validation Loss: 0.415691724071255
Epoch 284/500, Training Loss: 0.4478078237353681
Epoch 284/500, Validation Loss: 0.4154305218102096
Epoch 285/500, Training Loss: 0.4475826596668127
Epoch 285/500, Validation Loss: 0.4153842744115111
Epoch 286/500, Training Loss: 0.4474309954828429
Epoch 286/500, Validation Loss: 0.415172210761479
Epoch 287/500, Training Loss: 0.4472909999414555
Epoch 287/500, Validation Loss: 0.4151356630511098
Epoch 288/500, Training Loss: 0.44715507235507196
Epoch 288/500, Validation Loss: 0.4148810149787308
Epoch 289/500, Training Loss: 0.4470554817152751
Epoch 289/500, Validation Loss: 0.41471127720622275
Epoch 290/500, Training Loss: 0.446966205944466
Epoch 290/500, Validation Loss: 0.4145988047897042
Epoch 291/500, Training Loss: 0.4467664068763696
Epoch 291/500, Validation Loss: 0.41438182143421914
Epoch 292/500, Training Loss: 0.44678571369710807
Epoch 292/500, Validation Loss: 0.41409023628606423
Epoch 293/500, Training Loss: 0.44632479734460456
Epoch 293/500, Validation Loss: 0.41374537310042936
Epoch 294/500, Training Loss: 0.4461956106839332
Epoch 294/500, Validation Loss: 0.4136649424379522
Epoch 295/500, Training Loss: 0.4461782182800621
Epoch 295/500, Validation Loss: 0.4135436609193876
Epoch 296/500, Training Loss: 0.44597781287317634
Epoch 296/500, Validation Loss: 0.4133739308877425
Epoch 297/500, Training Loss: 0.44579886315434386
Epoch 297/500, Validation Loss: 0.4130807553792929
Epoch 298/500, Training Loss: 0.4456556380704438
Epoch 298/500, Validation Loss: 0.41285657186012764
Epoch 299/500, Training Loss: 0.44551402348062685
Epoch 299/500, Validation Loss: 0.41273336054442766
Epoch 300/500, Training Loss: 0.44541328031461214
Epoch 300/500, Validation Loss: 0.4125488458515762
Epoch 301/500, Training Loss: 0.4453360100228975
Epoch 301/500, Validation Loss: 0.4124042894933131
Epoch 302/500, Training Loss: 0.4451255292403053
Epoch 302/500, Validation Loss: 0.41225538547936974
Epoch 303/500, Training Loss: 0.44495196258145464
Epoch 303/500, Validation Loss: 0.41196850019615966
Epoch 304/500, Training Loss: 0.44488283896743835
Epoch 304/500, Validation Loss: 0.411621959952565
Epoch 305/500, Training Loss: 0.4447107948311159
Epoch 305/500, Validation Loss: 0.41144940528002655
Epoch 306/500, Training Loss: 0.4447102793046737
Epoch 306/500, Validation Loss: 0.41114147878312446
Epoch 307/500, Training Loss: 0.4444274616307591
Epoch 307/500, Validation Loss: 0.4110683094371449
Epoch 308/500, Training Loss: 0.4443264359740047
Epoch 308/500, Validation Loss: 0.4108715695994241
Epoch 309/500, Training Loss: 0.444238037139632
Epoch 309/500, Validation Loss: 0.4109044237570329
Epoch 310/500, Training Loss: 0.44409065033294953
Epoch 310/500, Validation Loss: 0.4107020249614468
Epoch 311/500, Training Loss: 0.4439893265703019
Epoch 311/500, Validation Loss: 0.4105905143471507
Epoch 312/500, Training Loss: 0.44375275222173843
Epoch 312/500, Validation Loss: 0.4103823385455392
Epoch 313/500, Training Loss: 0.443629885575477
Epoch 313/500, Validation Loss: 0.4100883339906668
Epoch 314/500, Training Loss: 0.44350982066786404
Epoch 314/500, Validation Loss: 0.4099409696343657
Epoch 315/500, Training Loss: 0.4433978827419625
Epoch 315/500, Validation Loss: 0.40971830912998747
Epoch 316/500, Training Loss: 0.4432118995097077
Epoch 316/500, Validation Loss: 0.40969335181372507
Epoch 317/500, Training Loss: 0.4431236712843304
Epoch 317/500, Validation Loss: 0.40939897143995607
Epoch 318/500, Training Loss: 0.4430560485334568
Epoch 318/500, Validation Loss: 0.40936782336854316
Epoch 319/500, Training Loss: 0.4429193757546758
Epoch 319/500, Validation Loss: 0.4092141029896674
Epoch 320/500, Training Loss: 0.44276138797051035
Epoch 320/500, Validation Loss: 0.40900828118448135
Epoch 321/500, Training Loss: 0.44256726585214906
Epoch 321/500, Validation Loss: 0.40901814730136427
Epoch 322/500, Training Loss: 0.4425513709633096
Epoch 322/500, Validation Loss: 0.40870833203390045
Epoch 323/500, Training Loss: 0.4423901424659274
Epoch 323/500, Validation Loss: 0.40858155411559266
Epoch 324/500, Training Loss: 0.4423179251352395
Epoch 324/500, Validation Loss: 0.4084298974507815
Epoch 325/500, Training Loss: 0.44217744647381874
Epoch 325/500, Validation Loss: 0.40803168152833913
Epoch 326/500, Training Loss: 0.4422090678745764
Epoch 326/500, Validation Loss: 0.40806587369411024
Epoch 327/500, Training Loss: 0.44194478201634674
Epoch 327/500, Validation Loss: 0.40760877225306125
Epoch 328/500, Training Loss: 0.4417879200345435
Epoch 328/500, Validation Loss: 0.4072635042202937
Epoch 329/500, Training Loss: 0.44165298698183236
Epoch 329/500, Validation Loss: 0.4070709556728214
Epoch 330/500, Training Loss: 0.4415797196462978
Epoch 330/500, Validation Loss: 0.4069433994107432
Epoch 331/500, Training Loss: 0.4414942939488034
Epoch 331/500, Validation Loss: 0.40684462174192654
Epoch 332/500, Training Loss: 0.44136333515177817
Epoch 332/500, Validation Loss: 0.4066720252687281
Epoch 333/500, Training Loss: 0.44126356603389144
Epoch 333/500, Validation Loss: 0.40652080441450145
Epoch 334/500, Training Loss: 0.44111490712582485
Epoch 334/500, Validation Loss: 0.40635497810004595
Epoch 335/500, Training Loss: 0.44112101885465577
Epoch 335/500, Validation Loss: 0.40615740147503937
Epoch 336/500, Training Loss: 0.44095212037206194
Epoch 336/500, Validation Loss: 0.40603568375884713
Epoch 337/500, Training Loss: 0.44089472814671044
Epoch 337/500, Validation Loss: 0.4058646002373138
Epoch 338/500, Training Loss: 0.44070295536898374
Epoch 338/500, Validation Loss: 0.4057204731099017
Epoch 339/500, Training Loss: 0.4406476414448552
Epoch 339/500, Validation Loss: 0.405723493981671
Epoch 340/500, Training Loss: 0.4405277381326884
Epoch 340/500, Validation Loss: 0.405537650956736
Epoch 341/500, Training Loss: 0.4404577151168236
Epoch 341/500, Validation Loss: 0.4052694405054117
Epoch 342/500, Training Loss: 0.44028086367162683
Epoch 342/500, Validation Loss: 0.4050452198301043
Epoch 343/500, Training Loss: 0.4402141362718008
Epoch 343/500, Validation Loss: 0.4049960850895225
Epoch 344/500, Training Loss: 0.440028209125574
Epoch 344/500, Validation Loss: 0.4048114651209348
Epoch 345/500, Training Loss: 0.4399076454682423
Epoch 345/500, Validation Loss: 0.4047174538884844
Epoch 346/500, Training Loss: 0.4397949124176195
Epoch 346/500, Validation Loss: 0.40452505009514944
Epoch 347/500, Training Loss: 0.43977123515450506
Epoch 347/500, Validation Loss: 0.4041635092202719
Epoch 348/500, Training Loss: 0.4395818040498252
Epoch 348/500, Validation Loss: 0.40401469654851146
Epoch 349/500, Training Loss: 0.43967403944235567
Epoch 349/500, Validation Loss: 0.4039324475573255
Epoch 350/500, Training Loss: 0.4393452393115807
Epoch 350/500, Validation Loss: 0.40375797121555773
Epoch 351/500, Training Loss: 0.4392511178319563
Epoch 351/500, Validation Loss: 0.4036404660769871
Epoch 352/500, Training Loss: 0.43912258971580687
Epoch 352/500, Validation Loss: 0.4035039381547408
Epoch 353/500, Training Loss: 0.43908534617513295
Epoch 353/500, Validation Loss: 0.40335162312953504
Epoch 354/500, Training Loss: 0.43905726598056444
Epoch 354/500, Validation Loss: 0.40304953866190724
Epoch 355/500, Training Loss: 0.4388838370902859
Epoch 355/500, Validation Loss: 0.40293838993295444
Epoch 356/500, Training Loss: 0.4387372681320788
Epoch 356/500, Validation Loss: 0.4026915899344853
Epoch 357/500, Training Loss: 0.4386416824945299
Epoch 357/500, Validation Loss: 0.40247940668812043
Epoch 358/500, Training Loss: 0.4386216199513778
Epoch 358/500, Validation Loss: 0.40223455467781466
Epoch 359/500, Training Loss: 0.4383860020878907
Epoch 359/500, Validation Loss: 0.4022113166072152
Epoch 360/500, Training Loss: 0.43839506178722304
Epoch 360/500, Validation Loss: 0.4022217656110788
Epoch 361/500, Training Loss: 0.4382397097621976
Epoch 361/500, Validation Loss: 0.4020851776971445
Epoch 362/500, Training Loss: 0.4381027631099942
Epoch 362/500, Validation Loss: 0.40187655989225807
Epoch 363/500, Training Loss: 0.43797616621843655
Epoch 363/500, Validation Loss: 0.40158087363490813
Epoch 364/500, Training Loss: 0.4379738403094129
Epoch 364/500, Validation Loss: 0.40141737925541865
Epoch 365/500, Training Loss: 0.4378100559632094
Epoch 365/500, Validation Loss: 0.4013044942509044
Epoch 366/500, Training Loss: 0.43767484581296884
Epoch 366/500, Validation Loss: 0.401064142004236
Epoch 367/500, Training Loss: 0.4376615140659427
Epoch 367/500, Validation Loss: 0.40092620447084504
Epoch 368/500, Training Loss: 0.4374929035338218
Epoch 368/500, Validation Loss: 0.4008411006494002
Epoch 369/500, Training Loss: 0.437419743461847
Epoch 369/500, Validation Loss: 0.4006964873957944
Epoch 370/500, Training Loss: 0.43730721133922573
Epoch 370/500, Validation Loss: 0.400634077462283
Epoch 371/500, Training Loss: 0.4372528324552118
Epoch 371/500, Validation Loss: 0.4004491343126669
Epoch 372/500, Training Loss: 0.43708679945971535
Epoch 372/500, Validation Loss: 0.40026491415965093
Epoch 373/500, Training Loss: 0.43702067329384253
Epoch 373/500, Validation Loss: 0.4000748779092516
Epoch 374/500, Training Loss: 0.4369883993627625
Epoch 374/500, Validation Loss: 0.3998936406977765
Epoch 375/500, Training Loss: 0.43687100706417914
Epoch 375/500, Validation Loss: 0.3996457178871353
Epoch 376/500, Training Loss: 0.4366476744124033
Epoch 376/500, Validation Loss: 0.3996491374133469
Epoch 377/500, Training Loss: 0.43666448507163463
Epoch 377/500, Validation Loss: 0.3994747508655895
Epoch 378/500, Training Loss: 0.4365158572317658
Epoch 378/500, Validation Loss: 0.3993060062457989
Epoch 379/500, Training Loss: 0.43644125323520455
Epoch 379/500, Validation Loss: 0.39919125414513923
Epoch 380/500, Training Loss: 0.4364035482545501
Epoch 380/500, Validation Loss: 0.3990274098786441
Epoch 381/500, Training Loss: 0.43635288881197387
Epoch 381/500, Validation Loss: 0.3990237894770387
Epoch 382/500, Training Loss: 0.4361732754127824
Epoch 382/500, Validation Loss: 0.39895314907098745
Epoch 383/500, Training Loss: 0.43611377952994984
Epoch 383/500, Validation Loss: 0.3988085768439553
Epoch 384/500, Training Loss: 0.4360099693194507
Epoch 384/500, Validation Loss: 0.39865403632064916
Epoch 385/500, Training Loss: 0.4359698905693509
Epoch 385/500, Validation Loss: 0.39847047878550246
Epoch 386/500, Training Loss: 0.43581715751124156
Epoch 386/500, Validation Loss: 0.39846199867013216
Epoch 387/500, Training Loss: 0.43572081896203896
Epoch 387/500, Validation Loss: 0.39842961593107745
Epoch 388/500, Training Loss: 0.43560723848795924
Epoch 388/500, Validation Loss: 0.3983780106940827
Epoch 389/500, Training Loss: 0.4355390157018389
Epoch 389/500, Validation Loss: 0.39816899616996965
Epoch 390/500, Training Loss: 0.43547622005916337
Epoch 390/500, Validation Loss: 0.39797004399361546
Epoch 391/500, Training Loss: 0.43534574958388583
Epoch 391/500, Validation Loss: 0.39778324497210515
Epoch 392/500, Training Loss: 0.43526931832468296
Epoch 392/500, Validation Loss: 0.3975224657492204
Epoch 393/500, Training Loss: 0.4351381460481146
Epoch 393/500, Validation Loss: 0.39748588707539945
Epoch 394/500, Training Loss: 0.43509371684096887
Epoch 394/500, Validation Loss: 0.3973544611559286
Epoch 395/500, Training Loss: 0.4349926323235944
Epoch 395/500, Validation Loss: 0.3971272679892453
Epoch 396/500, Training Loss: 0.4350297558282847
Epoch 396/500, Validation Loss: 0.39716775695998946
Epoch 397/500, Training Loss: 0.43484508731091154
Epoch 397/500, Validation Loss: 0.3970854220452247
Epoch 398/500, Training Loss: 0.4347568502786586
Epoch 398/500, Validation Loss: 0.39693094306177906
Epoch 399/500, Training Loss: 0.4347810338970775
Epoch 399/500, Validation Loss: 0.3967861734427415
Epoch 400/500, Training Loss: 0.4346004622819189
Epoch 400/500, Validation Loss: 0.3966597554745612
Epoch 401/500, Training Loss: 0.43448759952299143
Epoch 401/500, Validation Loss: 0.39651354915135867
Epoch 402/500, Training Loss: 0.43459784612576274
Epoch 402/500, Validation Loss: 0.3965845080939206
Epoch 403/500, Training Loss: 0.4343602698239473
Epoch 403/500, Validation Loss: 0.39638887210325763
Epoch 404/500, Training Loss: 0.43423100946349674
Epoch 404/500, Validation Loss: 0.39618872125427446
Epoch 405/500, Training Loss: 0.43427838498775573
Epoch 405/500, Validation Loss: 0.3960105130424747
Epoch 406/500, Training Loss: 0.4341826800169131
Epoch 406/500, Validation Loss: 0.396122081713243
Epoch 407/500, Training Loss: 0.4340890758411233
Epoch 407/500, Validation Loss: 0.39604603392737253
Epoch 408/500, Training Loss: 0.4339658922816778
Epoch 408/500, Validation Loss: 0.39596197512242703
Epoch 409/500, Training Loss: 0.4339062441941273
Epoch 409/500, Validation Loss: 0.39564410742227135
Epoch 410/500, Training Loss: 0.43373127089459423
Epoch 410/500, Validation Loss: 0.39549805281998274
Epoch 411/500, Training Loss: 0.4336464346261196
Epoch 411/500, Validation Loss: 0.3953108640460225
Epoch 412/500, Training Loss: 0.43362375006761694
Epoch 412/500, Validation Loss: 0.39519700911137967
Epoch 413/500, Training Loss: 0.4335054208559401
Epoch 413/500, Validation Loss: 0.39518431022569733
Epoch 414/500, Training Loss: 0.4335137465955811
Epoch 414/500, Validation Loss: 0.39501821607738347
Epoch 415/500, Training Loss: 0.4333132906415178
Epoch 415/500, Validation Loss: 0.39512499199285134
Epoch 416/500, Training Loss: 0.43331858364867765
Epoch 416/500, Validation Loss: 0.394945397779539
Epoch 417/500, Training Loss: 0.4332887722946569
Epoch 417/500, Validation Loss: 0.394797902602654
Epoch 418/500, Training Loss: 0.4332142945756198
Epoch 418/500, Validation Loss: 0.3945826938400021
Epoch 419/500, Training Loss: 0.43308173571603475
Epoch 419/500, Validation Loss: 0.39450942618506296
Epoch 420/500, Training Loss: 0.4329922693612341
Epoch 420/500, Validation Loss: 0.3944242596626282
Epoch 421/500, Training Loss: 0.4328684127281178
Epoch 421/500, Validation Loss: 0.39444843553877496
Epoch 422/500, Training Loss: 0.43284386570013844
Epoch 422/500, Validation Loss: 0.39430672743103723
Epoch 423/500, Training Loss: 0.4328667187078981
Epoch 423/500, Validation Loss: 0.39420679250320834
Epoch 424/500, Training Loss: 0.43271936670503736
Epoch 424/500, Validation Loss: 0.39394850235480766
Epoch 425/500, Training Loss: 0.43267182420923705
Epoch 425/500, Validation Loss: 0.3938559065391491
Epoch 426/500, Training Loss: 0.43259415323955835
Epoch 426/500, Validation Loss: 0.39368771306880107
Epoch 427/500, Training Loss: 0.43253661094856
Epoch 427/500, Validation Loss: 0.3935545350049997
Epoch 428/500, Training Loss: 0.4325866388381768
Epoch 428/500, Validation Loss: 0.3934274207461964
Epoch 429/500, Training Loss: 0.43246852194221275
Epoch 429/500, Validation Loss: 0.3934365209046896
Epoch 430/500, Training Loss: 0.4322664269379207
Epoch 430/500, Validation Loss: 0.39338390432394943
Epoch 431/500, Training Loss: 0.4321828404552563
Epoch 431/500, Validation Loss: 0.3932101269821068
Epoch 432/500, Training Loss: 0.43220420871524146
Epoch 432/500, Validation Loss: 0.39300600661859886
Epoch 433/500, Training Loss: 0.43225483920802354
Epoch 433/500, Validation Loss: 0.39299503201014035
Epoch 434/500, Training Loss: 0.431979605956481
Epoch 434/500, Validation Loss: 0.3930322496921985
Epoch 435/500, Training Loss: 0.4320866536767572
Epoch 435/500, Validation Loss: 0.3930425314934223
Epoch 436/500, Training Loss: 0.4318999654294383
Epoch 436/500, Validation Loss: 0.39282454685731366
Epoch 437/500, Training Loss: 0.43179907320111205
Epoch 437/500, Validation Loss: 0.3926726838985047
Epoch 438/500, Training Loss: 0.4317289588768836
Epoch 438/500, Validation Loss: 0.3926188891584223
Epoch 439/500, Training Loss: 0.43170095019135496
Epoch 439/500, Validation Loss: 0.3923377859127986
Epoch 440/500, Training Loss: 0.431602286937002
Epoch 440/500, Validation Loss: 0.39249302737124553
Epoch 441/500, Training Loss: 0.4316071201007343
Epoch 441/500, Validation Loss: 0.392387364591871
Epoch 442/500, Training Loss: 0.4314975080476885
Epoch 442/500, Validation Loss: 0.3921174283151503
Epoch 443/500, Training Loss: 0.43157043631463704
Epoch 443/500, Validation Loss: 0.39185000123915736
Epoch 444/500, Training Loss: 0.4314534111095698
Epoch 444/500, Validation Loss: 0.3918572559759214
Epoch 445/500, Training Loss: 0.43130110774638747
Epoch 445/500, Validation Loss: 0.39185329033182814
Epoch 446/500, Training Loss: 0.4312998411890207
Epoch 446/500, Validation Loss: 0.39185456718717304
Epoch 447/500, Training Loss: 0.4312145545479329
Epoch 447/500, Validation Loss: 0.3918465565551411
Epoch 448/500, Training Loss: 0.4312645626795606
Epoch 448/500, Validation Loss: 0.39154989611018787
Epoch 449/500, Training Loss: 0.43115933915282423
Epoch 449/500, Validation Loss: 0.3914140377725874
Epoch 450/500, Training Loss: 0.4310744033607134
Epoch 450/500, Validation Loss: 0.39122796136063415
Epoch 451/500, Training Loss: 0.43103726114173535
Epoch 451/500, Validation Loss: 0.39114547666017113
Epoch 452/500, Training Loss: 0.4310611724522838
Epoch 452/500, Validation Loss: 0.39107568542678633
Epoch 453/500, Training Loss: 0.43109826142182794
Epoch 453/500, Validation Loss: 0.3908963640788933
Epoch 454/500, Training Loss: 0.43093312333303085
Epoch 454/500, Validation Loss: 0.39078775319186126
Epoch 455/500, Training Loss: 0.43087670078886037
Epoch 455/500, Validation Loss: 0.3907568265091289
Epoch 456/500, Training Loss: 0.43083462441513176
Epoch 456/500, Validation Loss: 0.39083218574523926
Epoch 457/500, Training Loss: 0.4307166913759361
Epoch 457/500, Validation Loss: 0.3906476133829587
Epoch 458/500, Training Loss: 0.43066971175548274
Epoch 458/500, Validation Loss: 0.3906657378394882
Epoch 459/500, Training Loss: 0.43069146156063026
Epoch 459/500, Validation Loss: 0.3905422203726583
Epoch 460/500, Training Loss: 0.4305595084301476
Epoch 460/500, Validation Loss: 0.39049697967318747
Epoch 461/500, Training Loss: 0.43050019116490956
Epoch 461/500, Validation Loss: 0.3903553555538128
Epoch 462/500, Training Loss: 0.43048642249196645
Epoch 462/500, Validation Loss: 0.39024054965415556
Epoch 463/500, Training Loss: 0.4304868430105889
Epoch 463/500, Validation Loss: 0.39013102805459654
Epoch 464/500, Training Loss: 0.4303254275150008
Epoch 464/500, Validation Loss: 0.3901800118483506
Epoch 465/500, Training Loss: 0.4302852061698562
Epoch 465/500, Validation Loss: 0.38998605717312207
Epoch 466/500, Training Loss: 0.43037828234710246
Epoch 466/500, Validation Loss: 0.3898762104573188
Epoch 467/500, Training Loss: 0.43025077832410635
Epoch 467/500, Validation Loss: 0.38987230016039565
Epoch 468/500, Training Loss: 0.43024405642785907
Epoch 468/500, Validation Loss: 0.38977290283549915
Epoch 469/500, Training Loss: 0.43011821114074505
Epoch 469/500, Validation Loss: 0.38970049673860724
Epoch 470/500, Training Loss: 0.43011124587092087
Epoch 470/500, Validation Loss: 0.38964717960976936
Epoch 471/500, Training Loss: 0.4300682351675179
Epoch 471/500, Validation Loss: 0.38955035379954744
Epoch 472/500, Training Loss: 0.43021325922045395
Epoch 472/500, Validation Loss: 0.3895407582258249
Epoch 473/500, Training Loss: 0.42987886756327875
Epoch 473/500, Validation Loss: 0.38952484455975617
Epoch 474/500, Training Loss: 0.42989387033551146
Epoch 474/500, Validation Loss: 0.389487270798002
Epoch 475/500, Training Loss: 0.4298764634954946
Epoch 475/500, Validation Loss: 0.3893470667399369
Epoch 476/500, Training Loss: 0.42971659066276974
Epoch 476/500, Validation Loss: 0.38931651316679916
Epoch 477/500, Training Loss: 0.4297123760987337
Epoch 477/500, Validation Loss: 0.3892765242558021
Epoch 478/500, Training Loss: 0.4296343502654447
Epoch 478/500, Validation Loss: 0.3891202044951451
Epoch 479/500, Training Loss: 0.4296009899343102
Epoch 479/500, Validation Loss: 0.38903708349574695
Epoch 480/500, Training Loss: 0.42951175649023915
Epoch 480/500, Validation Loss: 0.3888803890773228
Epoch 481/500, Training Loss: 0.42939224273958415
Epoch 481/500, Validation Loss: 0.3890086447263693
Epoch 482/500, Training Loss: 0.4294375877067881
Epoch 482/500, Validation Loss: 0.38891664147377014
Epoch 483/500, Training Loss: 0.4293143634843264
Epoch 483/500, Validation Loss: 0.3888288709250363
Epoch 484/500, Training Loss: 0.4292818606726505
Epoch 484/500, Validation Loss: 0.388730847990358
Epoch 485/500, Training Loss: 0.42927148872043486
Epoch 485/500, Validation Loss: 0.38860889766123385
Epoch 486/500, Training Loss: 0.42917227447446277
Epoch 486/500, Validation Loss: 0.3885633728720925
Epoch 487/500, Training Loss: 0.429140035967225
Epoch 487/500, Validation Loss: 0.38873924489145156
Epoch 488/500, Training Loss: 0.429111365208224
Epoch 488/500, Validation Loss: 0.3884585275278463
Epoch 489/500, Training Loss: 0.42900773469518855
Epoch 489/500, Validation Loss: 0.3883880264573283
Epoch 490/500, Training Loss: 0.4290001481647465
Epoch 490/500, Validation Loss: 0.388312371520253
Epoch 491/500, Training Loss: 0.4289359867820793
Epoch 491/500, Validation Loss: 0.3882651077462481
Epoch 492/500, Training Loss: 0.42887672967950446
Epoch 492/500, Validation Loss: 0.3883101297663404
Epoch 493/500, Training Loss: 0.42884034489833034
Epoch 493/500, Validation Loss: 0.3883324745413545
Epoch 494/500, Training Loss: 0.4287902322622344
Epoch 494/500, Validation Loss: 0.3881165362023688
Epoch 495/500, Training Loss: 0.4289038920369459
Epoch 495/500, Validation Loss: 0.3878695353285059
Epoch 496/500, Training Loss: 0.42871517234306844
Epoch 496/500, Validation Loss: 0.3879598355912543
Epoch 497/500, Training Loss: 0.42868559486161656
Epoch 497/500, Validation Loss: 0.38783228242552126
Epoch 498/500, Training Loss: 0.4285578605784788
Epoch 498/500, Validation Loss: 0.38768426861081806
Epoch 499/500, Training Loss: 0.42849324893686874
Epoch 499/500, Validation Loss: 0.3875185104159566
Epoch 500/500, Training Loss: 0.42844379493829776
Epoch 500/500, Validation Loss: 0.38740388487840627
Classification Accuracy: 0.8516129032258064
Classification Report:
              precision    recall  f1-score   support

           0       0.87      0.88      0.88        93
           1       0.82      0.81      0.81        62

    accuracy                           0.85       155
   macro avg       0.85      0.84      0.85       155
weighted avg       0.85      0.85      0.85       155

/home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py:168: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load('best_model.pth'))
Exported graph: graph(%onnx::MatMul_0 : Float(8, strides=[1], requires_grad=0, device=cpu),
      %fc1.bias : Float(2, strides=[1], requires_grad=1, device=cpu),
      %onnx::MatMul_6 : Float(8, 2, strides=[1, 8], requires_grad=0, device=cpu)):
  %/fc1/MatMul_output_0 : Float(2, strides=[1], device=cpu) = onnx::MatMul[onnx_name="/fc1/MatMul"](%onnx::MatMul_0, %onnx::MatMul_6), scope: __main__.LinearClassifier::/torch.nn.modules.linear.Linear::fc1 # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117:0
  %5 : Float(2, strides=[1], requires_grad=1, device=cpu) = onnx::Add[onnx_name="/fc1/Add"](%fc1.bias, %/fc1/MatMul_output_0), scope: __main__.LinearClassifier::/torch.nn.modules.linear.Linear::fc1 # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117:0
  return (%5)

Model has been exported as datasets/concrete/split/concrete_tiny.onnx

Process finished with exit code 0
