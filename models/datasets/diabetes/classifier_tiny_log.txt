/usr/bin/python3.10 /home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py
/home/mustafa/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
Epoch 1/500, Training Loss: 0.670130524888385
Epoch 1/500, Validation Loss: 0.6628870532430452
Epoch 2/500, Training Loss: 0.6639123453552275
Epoch 2/500, Validation Loss: 0.6560905308559023
Epoch 3/500, Training Loss: 0.6589746509628367
Epoch 3/500, Validation Loss: 0.6507253914043821
Epoch 4/500, Training Loss: 0.6553238492660238
Epoch 4/500, Validation Loss: 0.6462500177580734
Epoch 5/500, Training Loss: 0.6527191061991331
Epoch 5/500, Validation Loss: 0.6434879693491705
Epoch 6/500, Training Loss: 0.6505470334706581
Epoch 6/500, Validation Loss: 0.6406105612886364
Epoch 7/500, Training Loss: 0.6490248608189588
Epoch 7/500, Validation Loss: 0.638636301303732
Epoch 8/500, Training Loss: 0.6475561805055572
Epoch 8/500, Validation Loss: 0.6367724681722706
Epoch 9/500, Training Loss: 0.6464225232490171
Epoch 9/500, Validation Loss: 0.6349923569580604
Epoch 10/500, Training Loss: 0.6453670939253695
Epoch 10/500, Validation Loss: 0.6338696212604128
Epoch 11/500, Training Loss: 0.6444171154299262
Epoch 11/500, Validation Loss: 0.6325535209014498
Epoch 12/500, Training Loss: 0.6435012089251584
Epoch 12/500, Validation Loss: 0.6311859909830422
Epoch 13/500, Training Loss: 0.6426011055319447
Epoch 13/500, Validation Loss: 0.6300924903359907
Epoch 14/500, Training Loss: 0.6417473036261688
Epoch 14/500, Validation Loss: 0.629054044855052
Epoch 15/500, Training Loss: 0.6409949174140419
Epoch 15/500, Validation Loss: 0.6278377031457836
Epoch 16/500, Training Loss: 0.640230060289692
Epoch 16/500, Validation Loss: 0.6267646530578876
Epoch 17/500, Training Loss: 0.639289006063614
Epoch 17/500, Validation Loss: 0.6258078638849587
Epoch 18/500, Training Loss: 0.6385456752066727
Epoch 18/500, Validation Loss: 0.624915614210326
Epoch 19/500, Training Loss: 0.6377939354330023
Epoch 19/500, Validation Loss: 0.6238918088633438
Epoch 20/500, Training Loss: 0.6369879929491039
Epoch 20/500, Validation Loss: 0.6229633339520159
Epoch 21/500, Training Loss: 0.6362395031270146
Epoch 21/500, Validation Loss: 0.6219734816715635
Epoch 22/500, Training Loss: 0.6356016209893608
Epoch 22/500, Validation Loss: 0.6211588999320721
Epoch 23/500, Training Loss: 0.6348885636311892
Epoch 23/500, Validation Loss: 0.620144962236799
Epoch 24/500, Training Loss: 0.6340933187935828
Epoch 24/500, Validation Loss: 0.6191226254249441
Epoch 25/500, Training Loss: 0.6332773075645418
Epoch 25/500, Validation Loss: 0.6183319595353357
Epoch 26/500, Training Loss: 0.6326570969276144
Epoch 26/500, Validation Loss: 0.6175291137448673
Epoch 27/500, Training Loss: 0.6320527385732981
Epoch 27/500, Validation Loss: 0.6165625956551782
Epoch 28/500, Training Loss: 0.6313315561141826
Epoch 28/500, Validation Loss: 0.6158315724339979
Epoch 29/500, Training Loss: 0.6305111616683405
Epoch 29/500, Validation Loss: 0.6150201826260008
Epoch 30/500, Training Loss: 0.6299501171325173
Epoch 30/500, Validation Loss: 0.614109100966618
Epoch 31/500, Training Loss: 0.6293097475831513
Epoch 31/500, Validation Loss: 0.6133662914407665
Epoch 32/500, Training Loss: 0.6285362399046204
Epoch 32/500, Validation Loss: 0.6125639594834427
Epoch 33/500, Training Loss: 0.6279171817351542
Epoch 33/500, Validation Loss: 0.6118082249986714
Epoch 34/500, Training Loss: 0.6275154392146531
Epoch 34/500, Validation Loss: 0.6107825051093924
Epoch 35/500, Training Loss: 0.6266249290613711
Epoch 35/500, Validation Loss: 0.610158476336249
Epoch 36/500, Training Loss: 0.6260199574562868
Epoch 36/500, Validation Loss: 0.6093985828860052
Epoch 37/500, Training Loss: 0.6253653460596749
Epoch 37/500, Validation Loss: 0.6086579160443668
Epoch 38/500, Training Loss: 0.6247775670092421
Epoch 38/500, Validation Loss: 0.607876849585566
Epoch 39/500, Training Loss: 0.6242022181356419
Epoch 39/500, Validation Loss: 0.6071785924763515
Epoch 40/500, Training Loss: 0.6238404564573334
Epoch 40/500, Validation Loss: 0.606340361052546
Epoch 41/500, Training Loss: 0.6230117555437141
Epoch 41/500, Validation Loss: 0.6057399346910673
Epoch 42/500, Training Loss: 0.6224304190117109
Epoch 42/500, Validation Loss: 0.6049529314041138
Epoch 43/500, Training Loss: 0.6219426314702026
Epoch 43/500, Validation Loss: 0.604321251655447
Epoch 44/500, Training Loss: 0.6212256258870414
Epoch 44/500, Validation Loss: 0.6035059176642319
Epoch 45/500, Training Loss: 0.620654322359593
Epoch 45/500, Validation Loss: 0.6027506147993023
Epoch 46/500, Training Loss: 0.6201257737877205
Epoch 46/500, Validation Loss: 0.6021236253195795
Epoch 47/500, Training Loss: 0.6196457384018924
Epoch 47/500, Validation Loss: 0.6014107547957321
Epoch 48/500, Training Loss: 0.6189902092935431
Epoch 48/500, Validation Loss: 0.6008384289412663
Epoch 49/500, Training Loss: 0.6184975026484086
Epoch 49/500, Validation Loss: 0.6001528707043878
Epoch 50/500, Training Loss: 0.6181489298019551
Epoch 50/500, Validation Loss: 0.5995855095057652
Epoch 51/500, Training Loss: 0.6175128146589801
Epoch 51/500, Validation Loss: 0.5987600622505977
Epoch 52/500, Training Loss: 0.6169831353192888
Epoch 52/500, Validation Loss: 0.5980193008636606
Epoch 53/500, Training Loss: 0.6163778716847439
Epoch 53/500, Validation Loss: 0.5973900823757566
Epoch 54/500, Training Loss: 0.6158406363741201
Epoch 54/500, Validation Loss: 0.5966409999748756
Epoch 55/500, Training Loss: 0.6153695588893286
Epoch 55/500, Validation Loss: 0.5962541637749508
Epoch 56/500, Training Loss: 0.614790349810261
Epoch 56/500, Validation Loss: 0.5955224684600172
Epoch 57/500, Training Loss: 0.6144126599283414
Epoch 57/500, Validation Loss: 0.5948418810449797
Epoch 58/500, Training Loss: 0.6139762944349364
Epoch 58/500, Validation Loss: 0.5943875107271918
Epoch 59/500, Training Loss: 0.6133575772439968
Epoch 59/500, Validation Loss: 0.5937624409280974
Epoch 60/500, Training Loss: 0.613216351363912
Epoch 60/500, Validation Loss: 0.5932738482952118
Epoch 61/500, Training Loss: 0.6124434080425809
Epoch 61/500, Validation Loss: 0.5924576007086655
Epoch 62/500, Training Loss: 0.6120767402471333
Epoch 62/500, Validation Loss: 0.5918128511001324
Epoch 63/500, Training Loss: 0.6115182734734519
Epoch 63/500, Validation Loss: 0.5912195403000404
Epoch 64/500, Training Loss: 0.6109787732418245
Epoch 64/500, Validation Loss: 0.590702328188666
Epoch 65/500, Training Loss: 0.6109705108948038
Epoch 65/500, Validation Loss: 0.5899730376128492
Epoch 66/500, Training Loss: 0.6101473322349775
Epoch 66/500, Validation Loss: 0.5896310405484562
Epoch 67/500, Training Loss: 0.6095891205292174
Epoch 67/500, Validation Loss: 0.589169886605493
Epoch 68/500, Training Loss: 0.6091973317624026
Epoch 68/500, Validation Loss: 0.5885774513770794
Epoch 69/500, Training Loss: 0.6087788953905443
Epoch 69/500, Validation Loss: 0.5879223963309979
Epoch 70/500, Training Loss: 0.6082451761989842
Epoch 70/500, Validation Loss: 0.587466533841758
Epoch 71/500, Training Loss: 0.607899576259059
Epoch 71/500, Validation Loss: 0.5869820210440405
Epoch 72/500, Training Loss: 0.6075670910059184
Epoch 72/500, Validation Loss: 0.5863415568039335
Epoch 73/500, Training Loss: 0.6069861205374506
Epoch 73/500, Validation Loss: 0.5859048674846518
Epoch 74/500, Training Loss: 0.6067443788606584
Epoch 74/500, Validation Loss: 0.5854110543070168
Epoch 75/500, Training Loss: 0.6062006314373549
Epoch 75/500, Validation Loss: 0.5848342848235163
Epoch 76/500, Training Loss: 0.6058454141714498
Epoch 76/500, Validation Loss: 0.584360076435681
Epoch 77/500, Training Loss: 0.6053480715725009
Epoch 77/500, Validation Loss: 0.583789206784347
Epoch 78/500, Training Loss: 0.6049723518627316
Epoch 78/500, Validation Loss: 0.5833477192911608
Epoch 79/500, Training Loss: 0.6045833231128572
Epoch 79/500, Validation Loss: 0.5828986959210758
Epoch 80/500, Training Loss: 0.6041798087693682
Epoch 80/500, Validation Loss: 0.5823141233674411
Epoch 81/500, Training Loss: 0.6037883000444878
Epoch 81/500, Validation Loss: 0.5817025334670626
Epoch 82/500, Training Loss: 0.6032821282994147
Epoch 82/500, Validation Loss: 0.5813183352865022
Epoch 83/500, Training Loss: 0.602928039303927
Epoch 83/500, Validation Loss: 0.5808316911089009
Epoch 84/500, Training Loss: 0.6027090727061977
Epoch 84/500, Validation Loss: 0.5803763136781496
Epoch 85/500, Training Loss: 0.6021831390799979
Epoch 85/500, Validation Loss: 0.5799781854810386
Epoch 86/500, Training Loss: 0.6017610533943389
Epoch 86/500, Validation Loss: 0.5793698173144768
Epoch 87/500, Training Loss: 0.6014300902240547
Epoch 87/500, Validation Loss: 0.5790151152117499
Epoch 88/500, Training Loss: 0.6010077607920272
Epoch 88/500, Validation Loss: 0.5785286919824009
Epoch 89/500, Training Loss: 0.6005553222457346
Epoch 89/500, Validation Loss: 0.5780220329761505
Epoch 90/500, Training Loss: 0.6001932067689061
Epoch 90/500, Validation Loss: 0.5775171364175862
Epoch 91/500, Training Loss: 0.5998823950188564
Epoch 91/500, Validation Loss: 0.5771487817682069
Epoch 92/500, Training Loss: 0.5995984675719973
Epoch 92/500, Validation Loss: 0.5766132885012133
Epoch 93/500, Training Loss: 0.5990861825214862
Epoch 93/500, Validation Loss: 0.5760654663217479
Epoch 94/500, Training Loss: 0.5987587689909412
Epoch 94/500, Validation Loss: 0.5757361856000177
Epoch 95/500, Training Loss: 0.598577670655881
Epoch 95/500, Validation Loss: 0.5751659068567999
Epoch 96/500, Training Loss: 0.5982894622191814
Epoch 96/500, Validation Loss: 0.5748548538520418
Epoch 97/500, Training Loss: 0.5976708357116586
Epoch 97/500, Validation Loss: 0.5743937965097099
Epoch 98/500, Training Loss: 0.5974682155490143
Epoch 98/500, Validation Loss: 0.5739205818751763
Epoch 99/500, Training Loss: 0.5972517359190147
Epoch 99/500, Validation Loss: 0.5736658470384006
Epoch 100/500, Training Loss: 0.5968312265930673
Epoch 100/500, Validation Loss: 0.5730162663706417
Epoch 101/500, Training Loss: 0.596492386619028
Epoch 101/500, Validation Loss: 0.5727407839791528
Epoch 102/500, Training Loss: 0.5959205170139477
Epoch 102/500, Validation Loss: 0.5722982698473437
Epoch 103/500, Training Loss: 0.5957172695040924
Epoch 103/500, Validation Loss: 0.5719661784583124
Epoch 104/500, Training Loss: 0.5953329521850501
Epoch 104/500, Validation Loss: 0.571373025918829
Epoch 105/500, Training Loss: 0.5950139943003876
Epoch 105/500, Validation Loss: 0.5710403076533613
Epoch 106/500, Training Loss: 0.5945954966589503
Epoch 106/500, Validation Loss: 0.5706538537452961
Epoch 107/500, Training Loss: 0.5943225287525348
Epoch 107/500, Validation Loss: 0.5702576431734808
Epoch 108/500, Training Loss: 0.5939880918080136
Epoch 108/500, Validation Loss: 0.5698279584276265
Epoch 109/500, Training Loss: 0.5937076099980033
Epoch 109/500, Validation Loss: 0.5695313698258894
Epoch 110/500, Training Loss: 0.5933130009436297
Epoch 110/500, Validation Loss: 0.5691120788968843
Epoch 111/500, Training Loss: 0.5930089819586699
Epoch 111/500, Validation Loss: 0.5686673460335567
Epoch 112/500, Training Loss: 0.5926654694022635
Epoch 112/500, Validation Loss: 0.568290551160944
Epoch 113/500, Training Loss: 0.5926394049681765
Epoch 113/500, Validation Loss: 0.5680968710060778
Epoch 114/500, Training Loss: 0.592070804627914
Epoch 114/500, Validation Loss: 0.5676329464747988
Epoch 115/500, Training Loss: 0.591690362498747
Epoch 115/500, Validation Loss: 0.5671891775624506
Epoch 116/500, Training Loss: 0.5914621179996256
Epoch 116/500, Validation Loss: 0.5669132758831156
Epoch 117/500, Training Loss: 0.5910873985823306
Epoch 117/500, Validation Loss: 0.5663940187158256
Epoch 118/500, Training Loss: 0.5908298487991817
Epoch 118/500, Validation Loss: 0.5660664932481174
Epoch 119/500, Training Loss: 0.5904748878221494
Epoch 119/500, Validation Loss: 0.56557186102045
Epoch 120/500, Training Loss: 0.5901808760907619
Epoch 120/500, Validation Loss: 0.5652873618849392
Epoch 121/500, Training Loss: 0.5898799758605673
Epoch 121/500, Validation Loss: 0.5649796426296234
Epoch 122/500, Training Loss: 0.5895801921907527
Epoch 122/500, Validation Loss: 0.5647237444746083
Epoch 123/500, Training Loss: 0.5895681137035235
Epoch 123/500, Validation Loss: 0.5644590258598328
Epoch 124/500, Training Loss: 0.5891060308593168
Epoch 124/500, Validation Loss: 0.563816178461601
Epoch 125/500, Training Loss: 0.5887655262840527
Epoch 125/500, Validation Loss: 0.5634893548899683
Epoch 126/500, Training Loss: 0.5883812541402252
Epoch 126/500, Validation Loss: 0.563259297403796
Epoch 127/500, Training Loss: 0.5881470848062185
Epoch 127/500, Validation Loss: 0.5629128176590492
Epoch 128/500, Training Loss: 0.5878301872221451
Epoch 128/500, Validation Loss: 0.5627035519172405
Epoch 129/500, Training Loss: 0.5876781786176302
Epoch 129/500, Validation Loss: 0.5623617151687885
Epoch 130/500, Training Loss: 0.5872415880251197
Epoch 130/500, Validation Loss: 0.5619985800364922
Epoch 131/500, Training Loss: 0.5869983477965414
Epoch 131/500, Validation Loss: 0.5615924596786499
Epoch 132/500, Training Loss: 0.5869085736558868
Epoch 132/500, Validation Loss: 0.5612822812179039
Epoch 133/500, Training Loss: 0.5865066649749514
Epoch 133/500, Validation Loss: 0.5608697461670843
Epoch 134/500, Training Loss: 0.5861348053596539
Epoch 134/500, Validation Loss: 0.5605960262232813
Epoch 135/500, Training Loss: 0.5859709047960392
Epoch 135/500, Validation Loss: 0.5603826549546472
Epoch 136/500, Training Loss: 0.585599835802501
Epoch 136/500, Validation Loss: 0.559973891439109
Epoch 137/500, Training Loss: 0.5853699291440584
Epoch 137/500, Validation Loss: 0.5595794803109663
Epoch 138/500, Training Loss: 0.5853730316934639
Epoch 138/500, Validation Loss: 0.559182477408442
Epoch 139/500, Training Loss: 0.5848020104278622
Epoch 139/500, Validation Loss: 0.5588678399036671
Epoch 140/500, Training Loss: 0.5845906394819974
Epoch 140/500, Validation Loss: 0.5585957529215977
Epoch 141/500, Training Loss: 0.5843590844276897
Epoch 141/500, Validation Loss: 0.5584126114845276
Epoch 142/500, Training Loss: 0.5842739161610382
Epoch 142/500, Validation Loss: 0.557925493552767
Epoch 143/500, Training Loss: 0.5837261609081006
Epoch 143/500, Validation Loss: 0.5578119210128126
Epoch 144/500, Training Loss: 0.5834651738127739
Epoch 144/500, Validation Loss: 0.5575108404817253
Epoch 145/500, Training Loss: 0.583311754343008
Epoch 145/500, Validation Loss: 0.5573210490160975
Epoch 146/500, Training Loss: 0.5829796896522493
Epoch 146/500, Validation Loss: 0.556780784294523
Epoch 147/500, Training Loss: 0.5829525018315519
Epoch 147/500, Validation Loss: 0.5566600633078608
Epoch 148/500, Training Loss: 0.5825094354219277
Epoch 148/500, Validation Loss: 0.5563268332645811
Epoch 149/500, Training Loss: 0.5823533465076425
Epoch 149/500, Validation Loss: 0.5559354414199961
Epoch 150/500, Training Loss: 0.5820966797501015
Epoch 150/500, Validation Loss: 0.5555524034746762
Epoch 151/500, Training Loss: 0.5817166018530421
Epoch 151/500, Validation Loss: 0.5553783651055961
Epoch 152/500, Training Loss: 0.5814861090267615
Epoch 152/500, Validation Loss: 0.5551409536394579
Epoch 153/500, Training Loss: 0.5812050129115248
Epoch 153/500, Validation Loss: 0.5547354447430578
Epoch 154/500, Training Loss: 0.5811773727282021
Epoch 154/500, Validation Loss: 0.5543412473695032
Epoch 155/500, Training Loss: 0.5807124138766383
Epoch 155/500, Validation Loss: 0.5542733289044479
Epoch 156/500, Training Loss: 0.5805112935977275
Epoch 156/500, Validation Loss: 0.5539863613145105
Epoch 157/500, Training Loss: 0.5802562274524398
Epoch 157/500, Validation Loss: 0.5536055369623776
Epoch 158/500, Training Loss: 0.5801369908579679
Epoch 158/500, Validation Loss: 0.5532843347253471
Epoch 159/500, Training Loss: 0.5798090003722207
Epoch 159/500, Validation Loss: 0.5530284006020119
Epoch 160/500, Training Loss: 0.5795177808465016
Epoch 160/500, Validation Loss: 0.5528192756504848
Epoch 161/500, Training Loss: 0.5793134796552818
Epoch 161/500, Validation Loss: 0.5526244362880444
Epoch 162/500, Training Loss: 0.579386586694522
Epoch 162/500, Validation Loss: 0.5523240422380382
Epoch 163/500, Training Loss: 0.5788586683557464
Epoch 163/500, Validation Loss: 0.5520073545390162
Epoch 164/500, Training Loss: 0.578626889756272
Epoch 164/500, Validation Loss: 0.5517308239279122
Epoch 165/500, Training Loss: 0.5784464235412342
Epoch 165/500, Validation Loss: 0.5515611716385546
Epoch 166/500, Training Loss: 0.5782120175645782
Epoch 166/500, Validation Loss: 0.5511647062054996
Epoch 167/500, Training Loss: 0.5779000031215519
Epoch 167/500, Validation Loss: 0.5509298608220857
Epoch 168/500, Training Loss: 0.5779571621999616
Epoch 168/500, Validation Loss: 0.5507860204269146
Epoch 169/500, Training Loss: 0.5774822159630404
Epoch 169/500, Validation Loss: 0.5505541994653899
Epoch 170/500, Training Loss: 0.5772798172366463
Epoch 170/500, Validation Loss: 0.5501974481960823
Epoch 171/500, Training Loss: 0.5770643304069171
Epoch 171/500, Validation Loss: 0.5500185890444393
Epoch 172/500, Training Loss: 0.5768342671891609
Epoch 172/500, Validation Loss: 0.5496077681409901
Epoch 173/500, Training Loss: 0.576587059200365
Epoch 173/500, Validation Loss: 0.5495435307765829
Epoch 174/500, Training Loss: 0.5763969935296412
Epoch 174/500, Validation Loss: 0.5492344704167597
Epoch 175/500, Training Loss: 0.5761904232550822
Epoch 175/500, Validation Loss: 0.5488543983163505
Epoch 176/500, Training Loss: 0.5759216773443382
Epoch 176/500, Validation Loss: 0.54870831452567
Epoch 177/500, Training Loss: 0.5758880777509963
Epoch 177/500, Validation Loss: 0.5482064454719938
Epoch 178/500, Training Loss: 0.5755637417958436
Epoch 178/500, Validation Loss: 0.5481095570942451
Epoch 179/500, Training Loss: 0.5752333187302175
Epoch 179/500, Validation Loss: 0.547933514775901
Epoch 180/500, Training Loss: 0.5751225920584837
Epoch 180/500, Validation Loss: 0.5478015778393581
Epoch 181/500, Training Loss: 0.574866151565502
Epoch 181/500, Validation Loss: 0.5475586899395647
Epoch 182/500, Training Loss: 0.5747930334267004
Epoch 182/500, Validation Loss: 0.5470725318481182
Epoch 183/500, Training Loss: 0.5743776699684185
Epoch 183/500, Validation Loss: 0.5469602860253433
Epoch 184/500, Training Loss: 0.5742083349485415
Epoch 184/500, Validation Loss: 0.546674470449316
Epoch 185/500, Training Loss: 0.5739915430878794
Epoch 185/500, Validation Loss: 0.5464783156740254
Epoch 186/500, Training Loss: 0.5737891443614853
Epoch 186/500, Validation Loss: 0.546318716016309
Epoch 187/500, Training Loss: 0.57366949686125
Epoch 187/500, Validation Loss: 0.54593481380364
Epoch 188/500, Training Loss: 0.5734507213979682
Epoch 188/500, Validation Loss: 0.5457882141244823
Epoch 189/500, Training Loss: 0.5732475481996767
Epoch 189/500, Validation Loss: 0.5455004603698336
Epoch 190/500, Training Loss: 0.5729160341026619
Epoch 190/500, Validation Loss: 0.5452509101094871
Epoch 191/500, Training Loss: 0.572717269594665
Epoch 191/500, Validation Loss: 0.5450049196851665
Epoch 192/500, Training Loss: 0.5726068592604312
Epoch 192/500, Validation Loss: 0.5448525445214634
Epoch 193/500, Training Loss: 0.5723329072114032
Epoch 193/500, Validation Loss: 0.5446068295117082
Epoch 194/500, Training Loss: 0.5721202320670726
Epoch 194/500, Validation Loss: 0.5442696758385362
Epoch 195/500, Training Loss: 0.5719517226325733
Epoch 195/500, Validation Loss: 0.544005942755732
Epoch 196/500, Training Loss: 0.5717650879472772
Epoch 196/500, Validation Loss: 0.5438356163172886
Epoch 197/500, Training Loss: 0.571581679072016
Epoch 197/500, Validation Loss: 0.543629370886704
Epoch 198/500, Training Loss: 0.5713443437546103
Epoch 198/500, Validation Loss: 0.5434599231029379
Epoch 199/500, Training Loss: 0.5711792930544421
Epoch 199/500, Validation Loss: 0.5431073760164196
Epoch 200/500, Training Loss: 0.5709543629953537
Epoch 200/500, Validation Loss: 0.5430476562730198
Epoch 201/500, Training Loss: 0.570735659679221
Epoch 201/500, Validation Loss: 0.5427947167692513
Epoch 202/500, Training Loss: 0.5705751212837532
Epoch 202/500, Validation Loss: 0.5425654002304735
Epoch 203/500, Training Loss: 0.5705015894642977
Epoch 203/500, Validation Loss: 0.5424381402032129
Epoch 204/500, Training Loss: 0.5702841790250782
Epoch 204/500, Validation Loss: 0.5421936316736813
Epoch 205/500, Training Loss: 0.5700046538862659
Epoch 205/500, Validation Loss: 0.5417975814178072
Epoch 206/500, Training Loss: 0.5698484763974806
Epoch 206/500, Validation Loss: 0.5416964066439661
Epoch 207/500, Training Loss: 0.5695823552000234
Epoch 207/500, Validation Loss: 0.5415517934437456
Epoch 208/500, Training Loss: 0.5695714380044067
Epoch 208/500, Validation Loss: 0.5413740277290344
Epoch 209/500, Training Loss: 0.5693519186485191
Epoch 209/500, Validation Loss: 0.5410968609924974
Epoch 210/500, Training Loss: 0.5690369736992891
Epoch 210/500, Validation Loss: 0.5408069008383257
Epoch 211/500, Training Loss: 0.5689075394937668
Epoch 211/500, Validation Loss: 0.5407399586562452
Epoch 212/500, Training Loss: 0.5687961469593439
Epoch 212/500, Validation Loss: 0.5404445611197373
Epoch 213/500, Training Loss: 0.5686126207062208
Epoch 213/500, Validation Loss: 0.5402849419363613
Epoch 214/500, Training Loss: 0.5686021057587096
Epoch 214/500, Validation Loss: 0.5399218407170526
Epoch 215/500, Training Loss: 0.5681001530124267
Epoch 215/500, Validation Loss: 0.5398772358894348
Epoch 216/500, Training Loss: 0.5680109584353712
Epoch 216/500, Validation Loss: 0.5395834846743222
Epoch 217/500, Training Loss: 0.567721563677548
Epoch 217/500, Validation Loss: 0.5394536955603237
Epoch 218/500, Training Loss: 0.5676869998653064
Epoch 218/500, Validation Loss: 0.5393043962018244
Epoch 219/500, Training Loss: 0.5674319319440888
Epoch 219/500, Validation Loss: 0.5390996326660288
Epoch 220/500, Training Loss: 0.5672183667259287
Epoch 220/500, Validation Loss: 0.5388795569025236
Epoch 221/500, Training Loss: 0.5670207830447724
Epoch 221/500, Validation Loss: 0.5385920528707833
Epoch 222/500, Training Loss: 0.5668285082838389
Epoch 222/500, Validation Loss: 0.53838711977005
Epoch 223/500, Training Loss: 0.5667300524547335
Epoch 223/500, Validation Loss: 0.538093928633065
Epoch 224/500, Training Loss: 0.5664576529568578
Epoch 224/500, Validation Loss: 0.5379639812584581
Epoch 225/500, Training Loss: 0.5665365767212553
Epoch 225/500, Validation Loss: 0.5376623314002464
Epoch 226/500, Training Loss: 0.5662940358760414
Epoch 226/500, Validation Loss: 0.5377071242907951
Epoch 227/500, Training Loss: 0.5659096006796569
Epoch 227/500, Validation Loss: 0.5374118562402397
Epoch 228/500, Training Loss: 0.5658919793489481
Epoch 228/500, Validation Loss: 0.5370761402722063
Epoch 229/500, Training Loss: 0.5656101405065597
Epoch 229/500, Validation Loss: 0.5370606985585443
Epoch 230/500, Training Loss: 0.5654533265689232
Epoch 230/500, Validation Loss: 0.5368759817090528
Epoch 231/500, Training Loss: 0.5653860671116209
Epoch 231/500, Validation Loss: 0.5366846323013306
Epoch 232/500, Training Loss: 0.5650695525734118
Epoch 232/500, Validation Loss: 0.5364518268355007
Epoch 233/500, Training Loss: 0.5650680899842048
Epoch 233/500, Validation Loss: 0.5361679052484447
Epoch 234/500, Training Loss: 0.5647013078632745
Epoch 234/500, Validation Loss: 0.5361887946211058
Epoch 235/500, Training Loss: 0.5645957633547721
Epoch 235/500, Validation Loss: 0.5358789316539107
Epoch 236/500, Training Loss: 0.5644932155058593
Epoch 236/500, Validation Loss: 0.5358466506004333
Epoch 237/500, Training Loss: 0.5642510678071105
Epoch 237/500, Validation Loss: 0.5356910341772539
Epoch 238/500, Training Loss: 0.5640111524521527
Epoch 238/500, Validation Loss: 0.535382966543066
Epoch 239/500, Training Loss: 0.5638600612263883
Epoch 239/500, Validation Loss: 0.5351502545948686
Epoch 240/500, Training Loss: 0.5637119605554549
Epoch 240/500, Validation Loss: 0.5349886499602219
Epoch 241/500, Training Loss: 0.5635506766579449
Epoch 241/500, Validation Loss: 0.5347297612963051
Epoch 242/500, Training Loss: 0.5635754884510289
Epoch 242/500, Validation Loss: 0.5345317885793489
Epoch 243/500, Training Loss: 0.5631705750078241
Epoch 243/500, Validation Loss: 0.5345318358519981
Epoch 244/500, Training Loss: 0.563012621034235
Epoch 244/500, Validation Loss: 0.5343975229509945
Epoch 245/500, Training Loss: 0.5628996649045962
Epoch 245/500, Validation Loss: 0.5341822248080681
Epoch 246/500, Training Loss: 0.5627341045125681
Epoch 246/500, Validation Loss: 0.5338427136684286
Epoch 247/500, Training Loss: 0.562511375607503
Epoch 247/500, Validation Loss: 0.5336711057301226
Epoch 248/500, Training Loss: 0.5624564258412942
Epoch 248/500, Validation Loss: 0.5335988792879828
Epoch 249/500, Training Loss: 0.5621990060895071
Epoch 249/500, Validation Loss: 0.5333206304188433
Epoch 250/500, Training Loss: 0.562152469402363
Epoch 250/500, Validation Loss: 0.5330970914199434
Epoch 251/500, Training Loss: 0.56221176525734
Epoch 251/500, Validation Loss: 0.5331036530692002
Epoch 252/500, Training Loss: 0.5618460635677174
Epoch 252/500, Validation Loss: 0.5328489809200682
Epoch 253/500, Training Loss: 0.5617999471099683
Epoch 253/500, Validation Loss: 0.5325306571763138
Epoch 254/500, Training Loss: 0.5614170930461067
Epoch 254/500, Validation Loss: 0.5323395914044874
Epoch 255/500, Training Loss: 0.5613131880538201
Epoch 255/500, Validation Loss: 0.5322842310214865
Epoch 256/500, Training Loss: 0.5610460733346211
Epoch 256/500, Validation Loss: 0.5320767225890324
Epoch 257/500, Training Loss: 0.5609987133930071
Epoch 257/500, Validation Loss: 0.5319197619783467
Epoch 258/500, Training Loss: 0.5607740758073618
Epoch 258/500, Validation Loss: 0.5317600287240127
Epoch 259/500, Training Loss: 0.5606279049061752
Epoch 259/500, Validation Loss: 0.5315389088515577
Epoch 260/500, Training Loss: 0.560506389238759
Epoch 260/500, Validation Loss: 0.53143844213979
Epoch 261/500, Training Loss: 0.5602830439750487
Epoch 261/500, Validation Loss: 0.5312791826396153
Epoch 262/500, Training Loss: 0.5601779541045816
Epoch 262/500, Validation Loss: 0.5310614448169182
Epoch 263/500, Training Loss: 0.5599681223348977
Epoch 263/500, Validation Loss: 0.5309205661559927
Epoch 264/500, Training Loss: 0.5598014405541801
Epoch 264/500, Validation Loss: 0.5307347229842482
Epoch 265/500, Training Loss: 0.5596828723752965
Epoch 265/500, Validation Loss: 0.5306611523546022
Epoch 266/500, Training Loss: 0.5595691480450125
Epoch 266/500, Validation Loss: 0.5304833424502405
Epoch 267/500, Training Loss: 0.5593416802931986
Epoch 267/500, Validation Loss: 0.5302336298186203
Epoch 268/500, Training Loss: 0.55919018704132
Epoch 268/500, Validation Loss: 0.530067030725808
Epoch 269/500, Training Loss: 0.5590578379799755
Epoch 269/500, Validation Loss: 0.5298729765004125
Epoch 270/500, Training Loss: 0.5589329900022325
Epoch 270/500, Validation Loss: 0.52983034479207
Epoch 271/500, Training Loss: 0.5587774077829258
Epoch 271/500, Validation Loss: 0.5296825417156877
Epoch 272/500, Training Loss: 0.5585787641491526
Epoch 272/500, Validation Loss: 0.5294121462723305
Epoch 273/500, Training Loss: 0.558407310001011
Epoch 273/500, Validation Loss: 0.5293189852402128
Epoch 274/500, Training Loss: 0.5583239609524746
Epoch 274/500, Validation Loss: 0.5292094802034313
Epoch 275/500, Training Loss: 0.5582367978726464
Epoch 275/500, Validation Loss: 0.5290991119269667
Epoch 276/500, Training Loss: 0.5581123440403291
Epoch 276/500, Validation Loss: 0.5288137392751102
Epoch 277/500, Training Loss: 0.557915815316987
Epoch 277/500, Validation Loss: 0.5286540851510805
Epoch 278/500, Training Loss: 0.5577340750942905
Epoch 278/500, Validation Loss: 0.5284530695142418
Epoch 279/500, Training Loss: 0.557524342998668
Epoch 279/500, Validation Loss: 0.5283841079679029
Epoch 280/500, Training Loss: 0.5574074554043775
Epoch 280/500, Validation Loss: 0.5282569301539454
Epoch 281/500, Training Loss: 0.5573769058595156
Epoch 281/500, Validation Loss: 0.5280449770647904
Epoch 282/500, Training Loss: 0.5571321997673596
Epoch 282/500, Validation Loss: 0.5279918703539618
Epoch 283/500, Training Loss: 0.5571888720745037
Epoch 283/500, Validation Loss: 0.5277125116052299
Epoch 284/500, Training Loss: 0.5568025985687582
Epoch 284/500, Validation Loss: 0.5276557114617578
Epoch 285/500, Training Loss: 0.5566746152979035
Epoch 285/500, Validation Loss: 0.5275136890082524
Epoch 286/500, Training Loss: 0.5567146484745281
Epoch 286/500, Validation Loss: 0.5274248534235461
Epoch 287/500, Training Loss: 0.5564994028937661
Epoch 287/500, Validation Loss: 0.5270154065099256
Epoch 288/500, Training Loss: 0.5562167258679978
Epoch 288/500, Validation Loss: 0.5269192621625703
Epoch 289/500, Training Loss: 0.5561645331773456
Epoch 289/500, Validation Loss: 0.5267216532394804
Epoch 290/500, Training Loss: 0.5560290942200957
Epoch 290/500, Validation Loss: 0.5267022724809318
Epoch 291/500, Training Loss: 0.5558251817355164
Epoch 291/500, Validation Loss: 0.5264605324843834
Epoch 292/500, Training Loss: 0.5557196938347773
Epoch 292/500, Validation Loss: 0.5262524341714794
Epoch 293/500, Training Loss: 0.5555203451568632
Epoch 293/500, Validation Loss: 0.5261634053855107
Epoch 294/500, Training Loss: 0.5556367066319429
Epoch 294/500, Validation Loss: 0.5262152419008058
Epoch 295/500, Training Loss: 0.5553928517096536
Epoch 295/500, Validation Loss: 0.525825584756917
Epoch 296/500, Training Loss: 0.5551417252426929
Epoch 296/500, Validation Loss: 0.5258101923712368
Epoch 297/500, Training Loss: 0.5550550401654767
Epoch 297/500, Validation Loss: 0.525554678563414
Epoch 298/500, Training Loss: 0.5549097752659903
Epoch 298/500, Validation Loss: 0.5254814871426287
Epoch 299/500, Training Loss: 0.5547336716225694
Epoch 299/500, Validation Loss: 0.5254603881260445
Epoch 300/500, Training Loss: 0.5546190395701531
Epoch 300/500, Validation Loss: 0.5251885107878981
Epoch 301/500, Training Loss: 0.5544219498749774
Epoch 301/500, Validation Loss: 0.5250949469105951
Epoch 302/500, Training Loss: 0.5544084790587647
Epoch 302/500, Validation Loss: 0.524849455932091
Epoch 303/500, Training Loss: 0.5543249218379541
Epoch 303/500, Validation Loss: 0.524606871193853
Epoch 304/500, Training Loss: 0.5539605100283631
Epoch 304/500, Validation Loss: 0.524610008659034
Epoch 305/500, Training Loss: 0.5538815829340972
Epoch 305/500, Validation Loss: 0.5245381326510988
Epoch 306/500, Training Loss: 0.5537720983032852
Epoch 306/500, Validation Loss: 0.524443034468026
Epoch 307/500, Training Loss: 0.5535857707428533
Epoch 307/500, Validation Loss: 0.524194964047136
Epoch 308/500, Training Loss: 0.5535086207993648
Epoch 308/500, Validation Loss: 0.5240285540449208
Epoch 309/500, Training Loss: 0.5534179249511306
Epoch 309/500, Validation Loss: 0.5239200448167736
Epoch 310/500, Training Loss: 0.5532450703713259
Epoch 310/500, Validation Loss: 0.5237010357708767
Epoch 311/500, Training Loss: 0.5531275613680898
Epoch 311/500, Validation Loss: 0.5237351625130094
Epoch 312/500, Training Loss: 0.5529474729266246
Epoch 312/500, Validation Loss: 0.523541140145269
Epoch 313/500, Training Loss: 0.5528909222151758
Epoch 313/500, Validation Loss: 0.5234044358648103
Epoch 314/500, Training Loss: 0.5527218584685796
Epoch 314/500, Validation Loss: 0.523297067346244
Epoch 315/500, Training Loss: 0.552573268861966
Epoch 315/500, Validation Loss: 0.5229788340371231
Epoch 316/500, Training Loss: 0.5524038191836196
Epoch 316/500, Validation Loss: 0.5228557997736437
Epoch 317/500, Training Loss: 0.5523083305447684
Epoch 317/500, Validation Loss: 0.522813703479438
Epoch 318/500, Training Loss: 0.5523447516466208
Epoch 318/500, Validation Loss: 0.5225798909006447
Epoch 319/500, Training Loss: 0.5521326333672862
Epoch 319/500, Validation Loss: 0.5224870864687294
Epoch 320/500, Training Loss: 0.5519507972443792
Epoch 320/500, Validation Loss: 0.5224919411642798
Epoch 321/500, Training Loss: 0.5517719229284389
Epoch 321/500, Validation Loss: 0.5222885999186285
Epoch 322/500, Training Loss: 0.5516500986932155
Epoch 322/500, Validation Loss: 0.5221239040637838
Epoch 323/500, Training Loss: 0.5515639364608396
Epoch 323/500, Validation Loss: 0.5219255850232881
Epoch 324/500, Training Loss: 0.5514131907644219
Epoch 324/500, Validation Loss: 0.5219019343113077
Epoch 325/500, Training Loss: 0.5512499334203909
Epoch 325/500, Validation Loss: 0.521775583768713
Epoch 326/500, Training Loss: 0.5511789853568405
Epoch 326/500, Validation Loss: 0.5216458871446806
Epoch 327/500, Training Loss: 0.5510171587462753
Epoch 327/500, Validation Loss: 0.5215177741544
Epoch 328/500, Training Loss: 0.5509967765661591
Epoch 328/500, Validation Loss: 0.521284793985301
Epoch 329/500, Training Loss: 0.5509966274435516
Epoch 329/500, Validation Loss: 0.5212934829037765
Epoch 330/500, Training Loss: 0.5506337303911063
Epoch 330/500, Validation Loss: 0.5210595408390308
Epoch 331/500, Training Loss: 0.5505607929531644
Epoch 331/500, Validation Loss: 0.520851646004052
Epoch 332/500, Training Loss: 0.5504086544282831
Epoch 332/500, Validation Loss: 0.5207423382791979
Epoch 333/500, Training Loss: 0.5503355377879222
Epoch 333/500, Validation Loss: 0.520637067227528
Epoch 334/500, Training Loss: 0.5501930129816635
Epoch 334/500, Validation Loss: 0.5204330888287775
Epoch 335/500, Training Loss: 0.550052085013807
Epoch 335/500, Validation Loss: 0.5202750512238207
Epoch 336/500, Training Loss: 0.5499680096654697
Epoch 336/500, Validation Loss: 0.5201259954222317
Epoch 337/500, Training Loss: 0.5498154633982
Epoch 337/500, Validation Loss: 0.5201104077799567
Epoch 338/500, Training Loss: 0.5499809024720218
Epoch 338/500, Validation Loss: 0.5199962237785603
Epoch 339/500, Training Loss: 0.549582809931295
Epoch 339/500, Validation Loss: 0.5199300009628822
Epoch 340/500, Training Loss: 0.5494016120331692
Epoch 340/500, Validation Loss: 0.519746735178191
Epoch 341/500, Training Loss: 0.5494785107935829
Epoch 341/500, Validation Loss: 0.5195040230093331
Epoch 342/500, Training Loss: 0.5492967912715685
Epoch 342/500, Validation Loss: 0.5195976536849449
Epoch 343/500, Training Loss: 0.5490961205803926
Epoch 343/500, Validation Loss: 0.5194096534416593
Epoch 344/500, Training Loss: 0.5489339251868995
Epoch 344/500, Validation Loss: 0.5193766293854549
Epoch 345/500, Training Loss: 0.5489111776902467
Epoch 345/500, Validation Loss: 0.519140387403554
Epoch 346/500, Training Loss: 0.5487145950230362
Epoch 346/500, Validation Loss: 0.5191008325280815
Epoch 347/500, Training Loss: 0.548663070525981
Epoch 347/500, Validation Loss: 0.5189035945925219
Epoch 348/500, Training Loss: 0.5484615741939296
Epoch 348/500, Validation Loss: 0.5187940042594383
Epoch 349/500, Training Loss: 0.5483942260511316
Epoch 349/500, Validation Loss: 0.5187431162801283
Epoch 350/500, Training Loss: 0.5482922812413903
Epoch 350/500, Validation Loss: 0.5185469584218387
Epoch 351/500, Training Loss: 0.5481070497626477
Epoch 351/500, Validation Loss: 0.5184196881179152
Epoch 352/500, Training Loss: 0.5479772191917874
Epoch 352/500, Validation Loss: 0.5183335296038923
Epoch 353/500, Training Loss: 0.5479465301254386
Epoch 353/500, Validation Loss: 0.5180666857752306
Epoch 354/500, Training Loss: 0.5477222465048067
Epoch 354/500, Validation Loss: 0.5180952456490747
Epoch 355/500, Training Loss: 0.5477459636036466
Epoch 355/500, Validation Loss: 0.5180980943400284
Epoch 356/500, Training Loss: 0.5476287280381059
Epoch 356/500, Validation Loss: 0.5177458298617396
Epoch 357/500, Training Loss: 0.5474423359892222
Epoch 357/500, Validation Loss: 0.5176940550064218
Epoch 358/500, Training Loss: 0.5473469949745378
Epoch 358/500, Validation Loss: 0.5175651106341131
Epoch 359/500, Training Loss: 0.5471435821367153
Epoch 359/500, Validation Loss: 0.5174342959091581
Epoch 360/500, Training Loss: 0.5471085339951116
Epoch 360/500, Validation Loss: 0.5172107127206079
Epoch 361/500, Training Loss: 0.5469868544981689
Epoch 361/500, Validation Loss: 0.5171664172205431
Epoch 362/500, Training Loss: 0.5468117447649744
Epoch 362/500, Validation Loss: 0.5172035981868875
Epoch 363/500, Training Loss: 0.5467214766383393
Epoch 363/500, Validation Loss: 0.5169759444121657
Epoch 364/500, Training Loss: 0.546644098265877
Epoch 364/500, Validation Loss: 0.5168657549496355
Epoch 365/500, Training Loss: 0.5466882708574362
Epoch 365/500, Validation Loss: 0.5169529092722925
Epoch 366/500, Training Loss: 0.5464298911600806
Epoch 366/500, Validation Loss: 0.516734963861005
Epoch 367/500, Training Loss: 0.5463282258927933
Epoch 367/500, Validation Loss: 0.5163674868386368
Epoch 368/500, Training Loss: 0.5461778495342816
Epoch 368/500, Validation Loss: 0.5164170799584225
Epoch 369/500, Training Loss: 0.5460070727923729
Epoch 369/500, Validation Loss: 0.5162148989480118
Epoch 370/500, Training Loss: 0.5459281614594841
Epoch 370/500, Validation Loss: 0.516132023827783
Epoch 371/500, Training Loss: 0.5458257459173433
Epoch 371/500, Validation Loss: 0.5159868885730875
Epoch 372/500, Training Loss: 0.5456137102187457
Epoch 372/500, Validation Loss: 0.515975388987311
Epoch 373/500, Training Loss: 0.5456162324275828
Epoch 373/500, Validation Loss: 0.5158141306762037
Epoch 374/500, Training Loss: 0.5454882786260636
Epoch 374/500, Validation Loss: 0.5158574293399679
Epoch 375/500, Training Loss: 0.5453707615201477
Epoch 375/500, Validation Loss: 0.5157025476981854
Epoch 376/500, Training Loss: 0.5452056553776704
Epoch 376/500, Validation Loss: 0.5154579918960045
Epoch 377/500, Training Loss: 0.545103956756201
Epoch 377/500, Validation Loss: 0.5153462475743787
Epoch 378/500, Training Loss: 0.5450102144113466
Epoch 378/500, Validation Loss: 0.5151904934439165
Epoch 379/500, Training Loss: 0.5449668015847658
Epoch 379/500, Validation Loss: 0.5150770781369045
Epoch 380/500, Training Loss: 0.5448385156288715
Epoch 380/500, Validation Loss: 0.5150143576079401
Epoch 381/500, Training Loss: 0.5447183174793947
Epoch 381/500, Validation Loss: 0.5147821224969009
Epoch 382/500, Training Loss: 0.5445443862421331
Epoch 382/500, Validation Loss: 0.5148217842496675
Epoch 383/500, Training Loss: 0.5445334421855778
Epoch 383/500, Validation Loss: 0.514534709782436
Epoch 384/500, Training Loss: 0.5445007767313241
Epoch 384/500, Validation Loss: 0.5144932331710026
Epoch 385/500, Training Loss: 0.5442205427745201
Epoch 385/500, Validation Loss: 0.5144693378744454
Epoch 386/500, Training Loss: 0.5443191182568975
Epoch 386/500, Validation Loss: 0.5142583518192686
Epoch 387/500, Training Loss: 0.5440528925015717
Epoch 387/500, Validation Loss: 0.514341079983218
Epoch 388/500, Training Loss: 0.5441394010957615
Epoch 388/500, Validation Loss: 0.5140574903323732
Epoch 389/500, Training Loss: 0.5438887158363669
Epoch 389/500, Validation Loss: 0.5141056673280124
Epoch 390/500, Training Loss: 0.5437543040880278
Epoch 390/500, Validation Loss: 0.5138398676083006
Epoch 391/500, Training Loss: 0.5435903044608275
Epoch 391/500, Validation Loss: 0.5138287862827038
Epoch 392/500, Training Loss: 0.5435004630568308
Epoch 392/500, Validation Loss: 0.5137728699322405
Epoch 393/500, Training Loss: 0.5433861538906559
Epoch 393/500, Validation Loss: 0.5136313243158932
Epoch 394/500, Training Loss: 0.5432620565762512
Epoch 394/500, Validation Loss: 0.5134914599615952
Epoch 395/500, Training Loss: 0.5431718700868902
Epoch 395/500, Validation Loss: 0.5133616739305956
Epoch 396/500, Training Loss: 0.5430464522576642
Epoch 396/500, Validation Loss: 0.5132333276600674
Epoch 397/500, Training Loss: 0.5429733347293385
Epoch 397/500, Validation Loss: 0.5132739533638132
Epoch 398/500, Training Loss: 0.5428279644950142
Epoch 398/500, Validation Loss: 0.5131297214277859
Epoch 399/500, Training Loss: 0.5427618480705461
Epoch 399/500, Validation Loss: 0.5130057602093138
Epoch 400/500, Training Loss: 0.5426231527239694
Epoch 400/500, Validation Loss: 0.512745123485039
Epoch 401/500, Training Loss: 0.5425308304015041
Epoch 401/500, Validation Loss: 0.51265372695594
Epoch 402/500, Training Loss: 0.5424470993615618
Epoch 402/500, Validation Loss: 0.5124266014016908
Epoch 403/500, Training Loss: 0.5424397135469057
Epoch 403/500, Validation Loss: 0.5125813987748377
Epoch 404/500, Training Loss: 0.542303671938969
Epoch 404/500, Validation Loss: 0.5122358408467523
Epoch 405/500, Training Loss: 0.5420658097253831
Epoch 405/500, Validation Loss: 0.5122241172297247
Epoch 406/500, Training Loss: 0.5421174141947783
Epoch 406/500, Validation Loss: 0.5120794516185234
Epoch 407/500, Training Loss: 0.5418698029589165
Epoch 407/500, Validation Loss: 0.5120640962288298
Epoch 408/500, Training Loss: 0.5418098810331782
Epoch 408/500, Validation Loss: 0.5119584809089529
Epoch 409/500, Training Loss: 0.5417222861249131
Epoch 409/500, Validation Loss: 0.5118649129209847
Epoch 410/500, Training Loss: 0.541569973836398
Epoch 410/500, Validation Loss: 0.5117351638859716
Epoch 411/500, Training Loss: 0.54157686954992
Epoch 411/500, Validation Loss: 0.511649599363064
Epoch 412/500, Training Loss: 0.5414817297702839
Epoch 412/500, Validation Loss: 0.5116816276106341
Epoch 413/500, Training Loss: 0.5412701981027699
Epoch 413/500, Validation Loss: 0.5113820824129828
Epoch 414/500, Training Loss: 0.5412748540357949
Epoch 414/500, Validation Loss: 0.5111709586505232
Epoch 415/500, Training Loss: 0.5413458127549241
Epoch 415/500, Validation Loss: 0.511265072329291
Epoch 416/500, Training Loss: 0.5410895304306925
Epoch 416/500, Validation Loss: 0.5111846420271643
Epoch 417/500, Training Loss: 0.5409700539746986
Epoch 417/500, Validation Loss: 0.511088429853834
Epoch 418/500, Training Loss: 0.5408563518435373
Epoch 418/500, Validation Loss: 0.5109167088722361
Epoch 419/500, Training Loss: 0.5406589583533659
Epoch 419/500, Validation Loss: 0.5107715684792091
Epoch 420/500, Training Loss: 0.540597525055848
Epoch 420/500, Validation Loss: 0.510780091943412
Epoch 421/500, Training Loss: 0.5404983356012313
Epoch 421/500, Validation Loss: 0.5105935252945999
Epoch 422/500, Training Loss: 0.540383852837916
Epoch 422/500, Validation Loss: 0.5105230047785002
Epoch 423/500, Training Loss: 0.5404277645897821
Epoch 423/500, Validation Loss: 0.510498448692519
Epoch 424/500, Training Loss: 0.5401275385469476
Epoch 424/500, Validation Loss: 0.51029398523528
Epoch 425/500, Training Loss: 0.540181904531724
Epoch 425/500, Validation Loss: 0.5101589776318649
Epoch 426/500, Training Loss: 0.5400350996235895
Epoch 426/500, Validation Loss: 0.5099742371460487
Epoch 427/500, Training Loss: 0.5398698095058818
Epoch 427/500, Validation Loss: 0.510014885458453
Epoch 428/500, Training Loss: 0.5397824164431411
Epoch 428/500, Validation Loss: 0.5097942866128067
Epoch 429/500, Training Loss: 0.5399280596156804
Epoch 429/500, Validation Loss: 0.5096276186663529
Epoch 430/500, Training Loss: 0.5396806235308975
Epoch 430/500, Validation Loss: 0.5095187703083301
Epoch 431/500, Training Loss: 0.5395138109418489
Epoch 431/500, Validation Loss: 0.5095340167653972
Epoch 432/500, Training Loss: 0.5393909147109843
Epoch 432/500, Validation Loss: 0.509440075734566
Epoch 433/500, Training Loss: 0.5392667170010466
Epoch 433/500, Validation Loss: 0.5093364510042914
Epoch 434/500, Training Loss: 0.5393192063274774
Epoch 434/500, Validation Loss: 0.5093715951360506
Epoch 435/500, Training Loss: 0.5393417078023516
Epoch 435/500, Validation Loss: 0.5091722165716106
Epoch 436/500, Training Loss: 0.5389808714944779
Epoch 436/500, Validation Loss: 0.509107892883235
Epoch 437/500, Training Loss: 0.5388963498882742
Epoch 437/500, Validation Loss: 0.5090776178343542
Epoch 438/500, Training Loss: 0.5387688123758057
Epoch 438/500, Validation Loss: 0.5088150480697895
Epoch 439/500, Training Loss: 0.5387597101804709
Epoch 439/500, Validation Loss: 0.5088733218867203
Epoch 440/500, Training Loss: 0.5385987309104238
Epoch 440/500, Validation Loss: 0.5087693426115759
Epoch 441/500, Training Loss: 0.5387264993492673
Epoch 441/500, Validation Loss: 0.5084864291651495
Epoch 442/500, Training Loss: 0.5386719231387979
Epoch 442/500, Validation Loss: 0.5085941789479091
Epoch 443/500, Training Loss: 0.5383297310972125
Epoch 443/500, Validation Loss: 0.5083106427357115
Epoch 444/500, Training Loss: 0.5382177816422958
Epoch 444/500, Validation Loss: 0.5081406502888121
Epoch 445/500, Training Loss: 0.5381155824861047
Epoch 445/500, Validation Loss: 0.5081227195673975
Epoch 446/500, Training Loss: 0.5380177724294822
Epoch 446/500, Validation Loss: 0.507988204216135
Epoch 447/500, Training Loss: 0.5379688517785383
Epoch 447/500, Validation Loss: 0.5080771364014725
Epoch 448/500, Training Loss: 0.5378458196890421
Epoch 448/500, Validation Loss: 0.5078931183650576
Epoch 449/500, Training Loss: 0.537781510717154
Epoch 449/500, Validation Loss: 0.507938968724218
Epoch 450/500, Training Loss: 0.5377548698828429
Epoch 450/500, Validation Loss: 0.5076999777349932
Epoch 451/500, Training Loss: 0.5375566672346446
Epoch 451/500, Validation Loss: 0.507488766620899
Epoch 452/500, Training Loss: 0.537441616173785
Epoch 452/500, Validation Loss: 0.507421637403554
Epoch 453/500, Training Loss: 0.5373749272894371
Epoch 453/500, Validation Loss: 0.5074461164145634
Epoch 454/500, Training Loss: 0.5372834527514723
Epoch 454/500, Validation Loss: 0.5073629309391153
Epoch 455/500, Training Loss: 0.5371821859029419
Epoch 455/500, Validation Loss: 0.5071615617850731
Epoch 456/500, Training Loss: 0.5371075836417839
Epoch 456/500, Validation Loss: 0.506994366645813
Epoch 457/500, Training Loss: 0.5369354618550235
Epoch 457/500, Validation Loss: 0.5069684817873198
Epoch 458/500, Training Loss: 0.5369464022487236
Epoch 458/500, Validation Loss: 0.5069528859237145
Epoch 459/500, Training Loss: 0.5367568480457896
Epoch 459/500, Validation Loss: 0.5067974760614592
Epoch 460/500, Training Loss: 0.536707231785332
Epoch 460/500, Validation Loss: 0.5068516762092196
Epoch 461/500, Training Loss: 0.5366880778271836
Epoch 461/500, Validation Loss: 0.5066207575386968
Epoch 462/500, Training Loss: 0.5364913508656749
Epoch 462/500, Validation Loss: 0.5066302336495498
Epoch 463/500, Training Loss: 0.5364742048181858
Epoch 463/500, Validation Loss: 0.5064310256777138
Epoch 464/500, Training Loss: 0.5364164897627449
Epoch 464/500, Validation Loss: 0.5064677030875765
Epoch 465/500, Training Loss: 0.5363203429199019
Epoch 465/500, Validation Loss: 0.5063433914348997
Epoch 466/500, Training Loss: 0.536170776329893
Epoch 466/500, Validation Loss: 0.5062718514738411
Epoch 467/500, Training Loss: 0.5360672138700716
Epoch 467/500, Validation Loss: 0.5061077218631218
Epoch 468/500, Training Loss: 0.5359899844537234
Epoch 468/500, Validation Loss: 0.5059399029304241
Epoch 469/500, Training Loss: 0.5359180239991769
Epoch 469/500, Validation Loss: 0.5059476741429033
Epoch 470/500, Training Loss: 0.5357715863755296
Epoch 470/500, Validation Loss: 0.505823684149775
Epoch 471/500, Training Loss: 0.5357323950451401
Epoch 471/500, Validation Loss: 0.505826171102195
Epoch 472/500, Training Loss: 0.5355806146476078
Epoch 472/500, Validation Loss: 0.5056007360589916
Epoch 473/500, Training Loss: 0.5355324522077038
Epoch 473/500, Validation Loss: 0.5055299226579995
Epoch 474/500, Training Loss: 0.5354932255807091
Epoch 474/500, Validation Loss: 0.5056073028465797
Epoch 475/500, Training Loss: 0.5353824437441551
Epoch 475/500, Validation Loss: 0.5053186724925863
Epoch 476/500, Training Loss: 0.5352055378244354
Epoch 476/500, Validation Loss: 0.5053168072782713
Epoch 477/500, Training Loss: 0.5351639752947418
Epoch 477/500, Validation Loss: 0.5052002545060783
Epoch 478/500, Training Loss: 0.5351059486191144
Epoch 478/500, Validation Loss: 0.5050102986138443
Epoch 479/500, Training Loss: 0.534970898978981
Epoch 479/500, Validation Loss: 0.5048720959959359
Epoch 480/500, Training Loss: 0.5349811805027157
Epoch 480/500, Validation Loss: 0.504910082652651
Epoch 481/500, Training Loss: 0.5348099385781883
Epoch 481/500, Validation Loss: 0.5048406966801348
Epoch 482/500, Training Loss: 0.5347956034946264
Epoch 482/500, Validation Loss: 0.5047342592272265
Epoch 483/500, Training Loss: 0.534700919129995
Epoch 483/500, Validation Loss: 0.5047121078803621
Epoch 484/500, Training Loss: 0.5345568646931781
Epoch 484/500, Validation Loss: 0.5044733573650492
Epoch 485/500, Training Loss: 0.5345767109309717
Epoch 485/500, Validation Loss: 0.5046012391304148
Epoch 486/500, Training Loss: 0.534335768422601
Epoch 486/500, Validation Loss: 0.5043925869053808
Epoch 487/500, Training Loss: 0.5342881425909934
Epoch 487/500, Validation Loss: 0.5041730547773426
Epoch 488/500, Training Loss: 0.5344836538730386
Epoch 488/500, Validation Loss: 0.5042633948654964
Epoch 489/500, Training Loss: 0.5341697083283181
Epoch 489/500, Validation Loss: 0.503877636687509
Epoch 490/500, Training Loss: 0.5339933189821865
Epoch 490/500, Validation Loss: 0.5039399471776239
Epoch 491/500, Training Loss: 0.5340220944841481
Epoch 491/500, Validation Loss: 0.5039302891698377
Epoch 492/500, Training Loss: 0.5338748982054783
Epoch 492/500, Validation Loss: 0.5037255266617084
Epoch 493/500, Training Loss: 0.5337121076535024
Epoch 493/500, Validation Loss: 0.5036685528426335
Epoch 494/500, Training Loss: 0.5337160339568581
Epoch 494/500, Validation Loss: 0.5036197366385624
Epoch 495/500, Training Loss: 0.5335683140008809
Epoch 495/500, Validation Loss: 0.5033923490294094
Epoch 496/500, Training Loss: 0.5335037530689488
Epoch 496/500, Validation Loss: 0.5033382197906231
Epoch 497/500, Training Loss: 0.5335906177925664
Epoch 497/500, Validation Loss: 0.503481597735964
Epoch 498/500, Training Loss: 0.5333665142068206
Epoch 498/500, Validation Loss: 0.5033071000000526
Epoch 499/500, Training Loss: 0.5333362216168054
Epoch 499/500, Validation Loss: 0.5031350275565838
Epoch 500/500, Training Loss: 0.5331678498168675
Epoch 500/500, Validation Loss: 0.5030837665344107
/home/mustafa/repos/counterfactuals/plausible_counterfactuals/train_classifier_datasets.py:168: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load('best_model.pth'))
Classification Accuracy: 0.7844827586206896
Classification Report:
              precision    recall  f1-score   support

           0       0.78      0.94      0.85        78
           1       0.78      0.47      0.59        38

    accuracy                           0.78       116
   macro avg       0.78      0.70      0.72       116
weighted avg       0.78      0.78      0.77       116

Exported graph: graph(%onnx::MatMul_0 : Float(8, strides=[1], requires_grad=0, device=cpu),
      %fc1.bias : Float(2, strides=[1], requires_grad=1, device=cpu),
      %onnx::MatMul_6 : Float(8, 2, strides=[1, 8], requires_grad=0, device=cpu)):
  %/fc1/MatMul_output_0 : Float(2, strides=[1], device=cpu) = onnx::MatMul[onnx_name="/fc1/MatMul"](%onnx::MatMul_0, %onnx::MatMul_6), scope: __main__.LinearClassifier::/torch.nn.modules.linear.Linear::fc1 # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117:0
  %5 : Float(2, strides=[1], requires_grad=1, device=cpu) = onnx::Add[onnx_name="/fc1/Add"](%fc1.bias, %/fc1/MatMul_output_0), scope: __main__.LinearClassifier::/torch.nn.modules.linear.Linear::fc1 # /home/mustafa/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117:0
  return (%5)

Model has been exported as datasets/diabetes/split/diabetes_tiny.onnx

Process finished with exit code 0
